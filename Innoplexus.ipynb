{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#import gensim\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Training_Data.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"Test_Data14.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e9a8166b84114aca147bf409f6f956635034c08</td>\n",
       "      <td>256 (previously stable on natalizumab), with 5...</td>\n",
       "      <td>fingolimod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e747e6822c867571afe7b907b51f0f2ca67b0e1a</td>\n",
       "      <td>On fingolimod and have been since December 201...</td>\n",
       "      <td>fingolimod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50b6d851bcff4f35afe354937949e9948975adf7</td>\n",
       "      <td>Apparently it's shingles! :-/ I do have a few ...</td>\n",
       "      <td>humira</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae</td>\n",
       "      <td>If the Docetaxel doing once a week x3 weeks th...</td>\n",
       "      <td>tagrisso</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  9e9a8166b84114aca147bf409f6f956635034c08   \n",
       "1  e747e6822c867571afe7b907b51f0f2ca67b0e1a   \n",
       "2  50b6d851bcff4f35afe354937949e9948975adf7   \n",
       "3  7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae   \n",
       "\n",
       "                                                text        drug  \n",
       "0  256 (previously stable on natalizumab), with 5...  fingolimod  \n",
       "1  On fingolimod and have been since December 201...  fingolimod  \n",
       "2  Apparently it's shingles! :-/ I do have a few ...      humira  \n",
       "3  If the Docetaxel doing once a week x3 weeks th...    tagrisso  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 697</th>\n",
       "      <th>Unnamed: 698</th>\n",
       "      <th>Unnamed: 699</th>\n",
       "      <th>Unnamed: 700</th>\n",
       "      <th>Unnamed: 701</th>\n",
       "      <th>Unnamed: 702</th>\n",
       "      <th>Unnamed: 703</th>\n",
       "      <th>Unnamed: 704</th>\n",
       "      <th>Unnamed: 705</th>\n",
       "      <th>Unnamed: 706</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0</td>\n",
       "      <td>Autoimmune diseases tend to come in clusters. ...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9eba8f80e7e20f3a2f48685530748fbfa95943e4</td>\n",
       "      <td>I can completely understand why youâd want t...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fe809672251f6bd0d986e00380f48d047c7e7b76</td>\n",
       "      <td>Interesting that it only targets S1P-1/5 recep...</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bd22104dfa9ec80db4099523e03fae7a52735eb6</td>\n",
       "      <td>Very interesting, grand merci. Now I wonder wh...</td>\n",
       "      <td>ocrevus</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b227688381f9b25e5b65109dd00f7f895e838249</td>\n",
       "      <td>Hi everybody, My latest MRI results for Brain ...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 707 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0   \n",
       "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4   \n",
       "2  fe809672251f6bd0d986e00380f48d047c7e7b76   \n",
       "3  bd22104dfa9ec80db4099523e03fae7a52735eb6   \n",
       "4  b227688381f9b25e5b65109dd00f7f895e838249   \n",
       "\n",
       "                                                text        drug sentiment  \\\n",
       "0  Autoimmune diseases tend to come in clusters. ...     gilenya         2   \n",
       "1  I can completely understand why youâd want t...     gilenya         2   \n",
       "2  Interesting that it only targets S1P-1/5 recep...  fingolimod         2   \n",
       "3  Very interesting, grand merci. Now I wonder wh...     ocrevus         2   \n",
       "4  Hi everybody, My latest MRI results for Brain ...     gilenya         1   \n",
       "\n",
       "  Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "      ...      Unnamed: 697 Unnamed: 698 Unnamed: 699 Unnamed: 700  \\\n",
       "0     ...               NaN          NaN          NaN          NaN   \n",
       "1     ...               NaN          NaN          NaN          NaN   \n",
       "2     ...               NaN          NaN          NaN          NaN   \n",
       "3     ...               NaN          NaN          NaN          NaN   \n",
       "4     ...               NaN          NaN          NaN          NaN   \n",
       "\n",
       "  Unnamed: 701 Unnamed: 702 Unnamed: 703 Unnamed: 704 Unnamed: 705  \\\n",
       "0          NaN          NaN          NaN          NaN          NaN   \n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "2          NaN          NaN          NaN          NaN          NaN   \n",
       "3          NaN          NaN          NaN          NaN          NaN   \n",
       "4          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "  Unnamed: 706  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 707 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = train[['unique_hash', 'text', 'drug','sentiment']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['sentiment'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-5d180c4728ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'unique_hash'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'drug'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2677\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2678\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2679\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2680\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2721\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2722\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2723\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2724\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1325\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[1;32m-> 1327\u001b[1;33m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['sentiment'] not in index\""
     ]
    }
   ],
   "source": [
    "new = test_df[['unique_hash', 'text', 'drug','sentiment']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0</td>\n",
       "      <td>Autoimmune diseases tend to come in clusters. ...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9eba8f80e7e20f3a2f48685530748fbfa95943e4</td>\n",
       "      <td>I can completely understand why youâd want t...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fe809672251f6bd0d986e00380f48d047c7e7b76</td>\n",
       "      <td>Interesting that it only targets S1P-1/5 recep...</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bd22104dfa9ec80db4099523e03fae7a52735eb6</td>\n",
       "      <td>Very interesting, grand merci. Now I wonder wh...</td>\n",
       "      <td>ocrevus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b227688381f9b25e5b65109dd00f7f895e838249</td>\n",
       "      <td>Hi everybody, My latest MRI results for Brain ...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0   \n",
       "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4   \n",
       "2  fe809672251f6bd0d986e00380f48d047c7e7b76   \n",
       "3  bd22104dfa9ec80db4099523e03fae7a52735eb6   \n",
       "4  b227688381f9b25e5b65109dd00f7f895e838249   \n",
       "\n",
       "                                                text        drug sentiment  \n",
       "0  Autoimmune diseases tend to come in clusters. ...     gilenya         2  \n",
       "1  I can completely understand why youâd want t...     gilenya         2  \n",
       "2  Interesting that it only targets S1P-1/5 recep...  fingolimod         2  \n",
       "3  Very interesting, grand merci. Now I wonder wh...     ocrevus         2  \n",
       "4  Hi everybody, My latest MRI results for Brain ...     gilenya         1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5315, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          3796\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           837\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           612\n",
       " and Guillain-BarrÃ© syndrome) have been reported; consider discontinuation of therapy if patient develops significant CNS reactions. â¢ Heart failure (HF): Use with caution in patients with mild HF (NYHA Class I                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         3\n",
       "  gefitinib                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   3\n",
       " IL: Personal communication. 10. Amgen/Pfizer Corporation. Enbrel (etanercept) for subcutaneous injection prescribing information. Thousand Oaks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              2\n",
       " 0.33â0.63; P < .001). A radiology review blinded-to-treatment arm resulted in similar outcomes. Median OS has not been reached. AEs leading to dose interruptions occurred in 35% of patients in the trametinib group and 22% of those in the chemotherapy group. AEs leading to dose reductions occurred in 27% of patients who received trametinib and in 10% of those who received chemotherapy. The most common AEs included rash                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      2\n",
       " maybe not us personally but I found a BusinessWire report on Global Age-Related Macular Degeneration Partnering Deals . They are hyping advice about buying into research and development of AMD treatments! They think people can make buckets of money off of us! Now                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      2\n",
       " in tabular form. [1907/19]  View answer Written answers (Question to Health)  Minister for Health Share I propose to take Questions Nos. 173 and 174 together. The number of employees in my Department at the end of December 2018 was 498 in terms of headcount. My Department does not hold records of all of the professional qualifications held by its employees. Some employees may be members of professional organisations in a personal capacity and pay their own membership fees. The Department would have no record of such memberships. The Department pays for corporate membership of the Institute of Public Administration. In 2018                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       2\n",
       " Ann Trans Med                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                2\n",
       " and has few symptoms. Also called indolent lymphoma. low-grade squamous intraepithelial lesion LSIL. A condition in which the cells of the uterine cervix are slightly abnormal. LSIL is not cancer. Also called LSIL. lower GI series X-rays of the colon and rectum that are taken after a person is given a barium enema. LSIL Low-grade squamous intraepithelial lesion. A condition in which the cells of the uterine cervix are slightly abnormal. LSIL is not cancer. Also called low-grade squamous intraepithelial lesion. LU 79553 An anticancer drug that kills cancer cells by affecting DNA synthesis. LU-103793 An anticancer drug that reduces the risk of tumor cell growth and reproduction. lubricant (LOO-brih-kant) An oily or slippery substance. lumbar puncture A procedure in which a needle is put into the lower part of the spinal column to collect cerebrospinal fluid or to give drugs. Also called spinal tap. lumen The cavity or channel within a tube or tubular organ such as a blood vessel or the intestine. lumpectomy (lum-PEK-toh-mee) Surgery to remove the tumor and a small amount of normal tissue around it. lung One of a pair of organs in the chest that supplies the body with oxygen       1\n",
       " constipation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
       "  seeds                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1\n",
       " and frequently they do not                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1\n",
       " resulting in inhibition of prostaglandin synthesis and prostaglandin-mediated inflammatory processes. Check for  active clinical trials using this agent. (  NCI Thesaurus )  eicosapentaenoic acid-enriched nutritional supplement A nutritional supplement enriched with eicosapentaenoic acid (EPA)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1\n",
       " see the Treatment Option Overview section. Previously Untreated Childhood Rhabdomyosarcoma The treatment of childhood  rhabdomyosarcoma often includes  surgery                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1\n",
       " Dec. 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1\n",
       " and renal and hepatic failure have been reported in premature neonates after receiving parenteral products containing polysorbate 80 (Alade 1986; CDC 1984). See manufacturer's labeling. Other warnings/precautions: â¢ Immunizations: Patients should be brought up to date with all immunizations before initiating therapy; live vaccines should not be given concurrently. There are no data available concerning the effects of therapy on vaccination or secondary transmission of live vaccines in patients receiving therapy.  Monitoring Parameters Monitor improvement of symptoms and physical function assessments. Latent TB screening prior to initiating and during therapy; signs/symptoms of active infection                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             1\n",
       " Lucchinetti CF                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1\n",
       " see the following: Colorectal Cancer Home Page Colorectal Cancer Prevention Colorectal Cancer Screening Tests to Detect Colorectal Cancer and Polyps Unusual Cancers of Childhood Treatment Cryosurgery in Cancer Treatment Drugs Approved for Colon and Rectal Cancer Targeted Cancer Therapies Genetic Testing for Hereditary Cancer Syndromes For general cancer information and other resources from the National Cancer Institute                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1\n",
       " ascites                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      1\n",
       " without increased clinical benefit. Therefore                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
       " patients and doctors from the Huntsman Cancer Institute took the day to raise awareness about sarcoma                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
       "â adding                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1\n",
       " 3d ed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
       " please visit www.clinicaltrials.gov ; for participation opportunities                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
       " median overall survival for standard of care is about 15 months. To put this in perspective                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1\n",
       " carb                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It mwans that unfortunately there are non numeric statements which need to be cleaned\n",
    "# I have to remove the rows where it is non numeric \n",
    "# or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = new[new['sentiment']==\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = new[new['sentiment']==\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b227688381f9b25e5b65109dd00f7f895e838249</td>\n",
       "      <td>Hi everybody, My latest MRI results for Brain ...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8fd3d7ad80791c9343e5cf8a83bd1adf6577d516</td>\n",
       "      <td>Why do you think that FIngolimod was such a mi...</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>127a56fe52ebb4ac4bd7c44882b38c90b309b0d1</td>\n",
       "      <td>I have no vision in one eye, unrelated to my e...</td>\n",
       "      <td>lucentis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>174806d157718c27217a1dc6ee66f0388a58710b</td>\n",
       "      <td>BMC Ophthalmol. 2019 Jan 8;19(1):8. Intravitre...</td>\n",
       "      <td>dexamethasone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 unique_hash  \\\n",
       "4   b227688381f9b25e5b65109dd00f7f895e838249   \n",
       "8   8fd3d7ad80791c9343e5cf8a83bd1adf6577d516   \n",
       "10  127a56fe52ebb4ac4bd7c44882b38c90b309b0d1   \n",
       "20  174806d157718c27217a1dc6ee66f0388a58710b   \n",
       "\n",
       "                                                 text           drug sentiment  \n",
       "4   Hi everybody, My latest MRI results for Brain ...        gilenya         1  \n",
       "8   Why do you think that FIngolimod was such a mi...     fingolimod         1  \n",
       "10  I have no vision in one eye, unrelated to my e...       lucentis         1  \n",
       "20  BMC Ophthalmol. 2019 Jan 8;19(1):8. Intravitre...  dexamethasone         1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = new[new['sentiment']==\"2\"]\n",
    "a3 = new[new['sentiment']==\"3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0</td>\n",
       "      <td>Autoimmune diseases tend to come in clusters. ...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9eba8f80e7e20f3a2f48685530748fbfa95943e4</td>\n",
       "      <td>I can completely understand why youâd want t...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fe809672251f6bd0d986e00380f48d047c7e7b76</td>\n",
       "      <td>Interesting that it only targets S1P-1/5 recep...</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bd22104dfa9ec80db4099523e03fae7a52735eb6</td>\n",
       "      <td>Very interesting, grand merci. Now I wonder wh...</td>\n",
       "      <td>ocrevus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0   \n",
       "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4   \n",
       "2  fe809672251f6bd0d986e00380f48d047c7e7b76   \n",
       "3  bd22104dfa9ec80db4099523e03fae7a52735eb6   \n",
       "\n",
       "                                                text        drug sentiment  \n",
       "0  Autoimmune diseases tend to come in clusters. ...     gilenya         2  \n",
       "1  I can completely understand why youâd want t...     gilenya         2  \n",
       "2  Interesting that it only targets S1P-1/5 recep...  fingolimod         2  \n",
       "3  Very interesting, grand merci. Now I wonder wh...     ocrevus         2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((612, 4), (837, 4), (3796, 4))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape,a1.shape,a2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [a,a1,a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(df_list, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>be5a13376933a7f9bbf8e801c31691092f63260a</td>\n",
       "      <td>Reply posted for JessZidek. Hi Jess Sorry to r...</td>\n",
       "      <td>humira</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e8f39b50683bb1b94689e8e462bdcd1aff331ee1</td>\n",
       "      <td>Last Updated: January 01, 2017.  Share | Comme...</td>\n",
       "      <td>ocrelizumab</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c2df2a0e70805bb1a17305e2ac137aeae26d424a</td>\n",
       "      <td>Hi I was on rebif for about a year â rotate ...</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>603c2f1612eeabcaac016b6da0df4117b6a8ccd8</td>\n",
       "      <td>No problem. I know how hard and lonely this jo...</td>\n",
       "      <td>tecentriq</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>918b3d83f17c334962f30974f2ff1f16630eebf0</td>\n",
       "      <td>Conclusion: These real-life results suggest th...</td>\n",
       "      <td>dexamethasone implant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  be5a13376933a7f9bbf8e801c31691092f63260a   \n",
       "1  e8f39b50683bb1b94689e8e462bdcd1aff331ee1   \n",
       "2  c2df2a0e70805bb1a17305e2ac137aeae26d424a   \n",
       "3  603c2f1612eeabcaac016b6da0df4117b6a8ccd8   \n",
       "4  918b3d83f17c334962f30974f2ff1f16630eebf0   \n",
       "\n",
       "                                                text                   drug  \\\n",
       "0  Reply posted for JessZidek. Hi Jess Sorry to r...                 humira   \n",
       "1  Last Updated: January 01, 2017.  Share | Comme...            ocrelizumab   \n",
       "2  Hi I was on rebif for about a year â rotate ...             fingolimod   \n",
       "3  No problem. I know how hard and lonely this jo...              tecentriq   \n",
       "4  Conclusion: These real-life results suggest th...  dexamethasone implant   \n",
       "\n",
       "  sentiment  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5245, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5245, 4), (5245, 4))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape,df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above thing confirms that there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = df2['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Reply posted for JessZidek. Hi Jess Sorry to r...\n",
       "1    Last Updated: January 01, 2017.  Share | Comme...\n",
       "2    Hi I was on rebif for about a year â rotate ...\n",
       "3    No problem. I know how hard and lonely this jo...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1['Text'] = test_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Text'] = df2['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2924, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 (previously stable on natalizumab), with 55% switching to fingolimod\n",
      "==================================================\n",
      "Hi Folks, Can anyone share the experiences with combo yervoy + Keytruda ? My mom is starting her first infusion next week. I am really scared and making false wishes that I can escape this but the truth is I have to walk this path with mom to her recovery. She is so healthy I am afraid this will put her down. Please share experiences.\n",
      "==================================================\n",
      "@fxms , Iâm not sure what more Roche can do. Itâs now becoming a bit of a âbuggerâs muddleâ. If NICE had agreed a differential pricing for PPMS, how would that be managed? And, would there be a temptation to âcook the booksââ¦â¦. Surely no\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# printing some random reviews\n",
    "sent_0 = final1['Text'].values[0]\n",
    "print(sent_0)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_1000 = final1['Text'].values[1000]\n",
    "print(sent_1000)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_1500 = final1['Text'].values[1500]\n",
    "print(sent_1500)\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply posted for JessZidek. Hi Jess Sorry to read about the challenges you are having with your health. You mentioned a lot in your post. I just want to share some info on a few of the points. First, I know you said that you are scared of Humira. Humira and other biologics are very successful in reducing symptoms and inducing and maintain disease remission. To reduce your level of fear it can help to learn more about your treatment option. You can learn more about some of your treatment options. To learn more view our Understanding IBD Medication brochure at: http://www.crohnscolitisfoundation.org/assets/pdfs/understanding-ibd-meds-nov.pdf . If you would like to talk, contact the Help Center at 888-694-8872 or at info@crohnscolitisfoundation.org\n",
      "==================================================\n",
      "I have my 2nd Entyvio infusion the day after tomorrow. Have had a tiny little setback -- 3 days after my 1st infusion, I developed a bladder infection, so was put on cipro for a week, which I just finished yesterday. Have been having a little bit of stomach pains -- nothing severe, but enough to let me know that this flare is not over yet. Mostly doing fine, still only 1-2 BMs a day, formed. A trace of blood here and there, and very mild cramping. Definitely could be the Cipro. Sat. night, I felt really rough with upper abdominal pain, more heartburn-ish, but I was also bad that day and had some chips and nacho cheese and then some apple pie with ice cream for dessert. (it's so hard to resist some times!!!) Have been using Canasa suppositories every night. Waiting on my Uceris foam, which I got the okay on from Salix (no cost to me!!!), and they are delivering that on Wed. Hoping to start weaning off Prednisone when that comes. I went down to 25 mg, but haven't been able to go lower than that yet. Also, been *really* tired the last few days. Have had to take some naps, and I never, ever nap. This weekend, I was so tired, I was just draggy, and didn't have the energy to do anything except sit. Also been having a lot of headaches, which have not been my typical right-sided migraine. These were more forehead/sinus/top of the head, and my migraine meds didn't touch them. Wondering if the tiredness and headaches are related to Entyvio. I expected that to only last a couple of days after the infusion, not a prolonged reaction. So, we'll see what happens after this next one. Wondering if the UTI was related to Entyvio infusion/lower immunity, or just coincidence. Hoping just coincidental. Partial Colectomy for diverticulitis Sept 2014 UC Diagnosis March 2016 - 18 days in hospital with pancolitis Apriso .375 g x 4/day Canasa suppositories as needed Methotrexate 2.5 mg x 4/week Levothyroxine .50 mcg/day Zoloft for depression Supplements: Zinc, L-glutamine, VSL #3, Folic Acid, CoQ10, turmeric, Started Entyvio Wed., 3/29/17 Constant flares since March 2016 only relieved by Prednisone\n",
      "==================================================\n",
      "Thank you, Merry. You are a world of knowledge and support. I actually joined a Chemo Support group on FB and everyone shares knowledge and experience. I did get a wig a couple of weeks ago and ordered a couple of the Chemo caps, which look horrible on me! Guess I can wear them around the house. I need more poof. Have read it is better to buzz your head rather then shave it. Yes, this is very hard as I am one that curls and styles my hair every day. Now, I haven't washed it for a week. I guess, since I have not been given many options for treatment, this almost seems futile but I do not want to die any time soon. I would talk to Wakekee before I make any decisions. Was thinking of maybe going to Scripps but, again, would be difficult and I hate to be far from home. Will see what he says this morning.  Jump to this post Saw my dictir this morning. My Neutrophols are only .8! He us skipping Chemo this week, will recheck blood work on Tuesday, Chemo next week for 3 days the 5 days of Granix injections. Going to try and schedule me to get a port put in (shudder) then a PET scan. I was going to ask him how he thought I was doing but guess he can't answer that until after the scan. Asked him about the pains I am getting and he didn't know why. Had pain in my cgest and by my left arm pit this morning. Hard to believe he has done this fir so many years and has no answers. To,d him I found a couple of posts where people with this LCNELC are also getting Keytruda or Opvido but he says that is immunotherapy and he and Wakelee are wary of trying that on me because, this undiagnosed, they are sure I have some immune probkems going on since before this cancer diagnosis. Guess it is a wait and see. Wish he were more talkative.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today was awesome. The Great Scott solved all of my problems. We have a great treatment plan devised that is sure to bring me to new levels of health and wellness. I went in hopeless and I re-emerged energized! Full of hope. MS does NOT have me! Okâ¦Iâm just fucking with you. Thatâs not how today went at all. Today wasâ¦a lot. Iâm going to try and break it down in such a way that wonât result in The Great Scott getting hate mail. I must stress up front, that I trust this man. I know he wants whatâs best for me. Iâm in his category of âdifficultâ cases and he seems dedicated to working with me to get me to some kind of stability. While I do not doubt his intentions for even a single second, nor do I doubt his knowledge or skill or experience, I do wish heâd work on his words a bit. His non-medical jargon words are not terribly helpful. His words areâ¦just off. The thing is, if he did that, if he got better with his words then these posts wouldnât be so entertaining, so in a way heâs helping me even when heâs not helping me! Right? Not exactly. I was twenty minutes late to my appointment because of the construction going on in the parking garage at TGSâs office. I had to walk far (very far) to get to his office. By the time I got there, 20 minutes late, I could barely stand up. He was very gracious about my late arrival â wasnât perturbed at all. He was delighted to see me â but noted Stanley right off the bat. âSo, things havenât gotten any better, it would appear? Letâs see you walk. Come with me,â and he guided me into the hallway. I walked about 20 feet, slowly and stumbling. âNow letâs get you out into the hallway.â I looked at him like Iâd kill him dead with my own two hands, or with Stanley, but what choice did I have? Off we went. He measured it out in advance, I suppose, heâs clearly done this bit with patients before. He tskâed and nodded while walking just slightly behind me. Then we got back to the exam room and he asked me to grab Stanley and weâd do it all again! Hooray! Sweet baby Jesus Christ on a cracker. By this time, I am openly stumbling. But my speed does improve a tad with Stanley and TGS seems pleased. Then the words started. The talking began, again. TGS: So the inactivity has led to some weight gain I can see it. How much weight have you gained, Maribeth? ME: Umâ¦I have no Idea. I havenât weighed myself in 20 years. TGS: Until now â get out here, Iâm weighing you right now. Thatâs ridiculous. You are not a ridiculous person, Maribeth. (He is very wrong about this but onward)  ME: Ok, fine, but do NOT tell me what the scale says. Trust me on this. I have major body image, weight-related issues and you might think I can handle this but trust me when I tell you I probably cannot. TGS: Ok, Maribeth weâll do it your way but youâre getting on the scale. This might be a good time to explain why I havenât weighed myself in over 20 years. I have been in therapy for a very long time. The reasons are diverse and vast, no doubt, but one of the biggest reasons is that Iâve had lifelong issues with body image and weight. To make a long, painful & a little bit ridiculous history short, suffice to say that Iâd somehow gotten it into my head that I was too pretty to be overweight. While I was never âfatâ per se, I was also never thin. I remember being told by my long-term college boyfriend how Iâd be âthe prettiest girl on campusâ if I lost 10 pounds. Then later in life my mother-in-law was openly disdainful of my less than skinny figure. She thought her son deserved better. Her son didnât agree with her, but this didnât really help me out so much. That womanâs disdain scarred me. Throughout my teens, young adulthood and even into my 40s I was obsessed with the idea that the worst thing I could ever be was to be fat. Another boyfriend, this one sobbed and sobbed when I finally broke up with him, he found an old picture of me from a few years prior while on a trip with a friend to Key West and he held it up in front of me and said, âSee? You CAN be thin, you just donât want to be thin bad enough.â Suffice to say, he wasnât my boyfriend for long after that but he was my boyfriend for longer than he should have been. Itâs a testament to my poor self-esteem that I didnât kick him out on the spot. The truth is, I thought he was right. I was ashamed of my inability to be skinny. He also had no idea of the extremes I had gone to throughout my life to starve myself into submission. Drugs were involved. Scary habits were involved. Without spilling all of my dirty laundry here on the internets, you can rest assured that I went to every single extreme you are imagining right now in my pursuit of thinness, and probably a few extreme measures you havenât thought of yet. I was too pretty to be fat. You canât be pretty AND fat. If I wasnât pretty, pleasing and attractive to men I was worthless. No matter how successful I was or funny or smart there would always be that one thing I could never seem to be and that was thin. Or thin enough? Iâve been in therapy for 20 years folks. I know my issues inside and out and this notion that I am somehow unacceptable for not being perfectly svelte is at the root of very many of my many other issues. Itâs like the Mack Daddy of my issues. The OG issue. The original vampire of issues that sucked the joy out of my life like so much young, innocent blood for most if not all of my life. Iâm 51 years old now. Iâve worked long and hard to put my fear of fat in its place. Iâm never completely over it, but Iâm so much better now than Iâve ever been. Iâm carrying some extra pounds right now. I know this because of my clothes and how they fit, or donât fit, as the case may be. Iâve told myself over and over again that I need to stop falling into old habits and allowing myself to feel inferior or somehow less acceptable because Iâm carrying a few extra pounds. Cheryl, my therapist, at one point in my mid-30s forbade me to even utter the word âdiet.â Focus on diet isnât good for my mental health (or my physical health). Iâve spent many years working this out. I was a little bigger than I wanted to be, sure, but I was also healthy. So, fuck society and the annoying expectations of most (white) men Iâve had the pleasure of being in relationships with. I was OK with me just the way I was. Kind of. Sometimes. I work hard at it and I mostly fail, but I keep trying because I think itâs important so I am mostly ok. Until Iâm not. TGS: Maribeth, we have to take some novel approaches to try to figure out whatâs going on with you. The first thing Iâm going to ask you to do is to lose 25 to 30 pounds quickly. I know that sounds extreme, but we need to see how you do with your walking and weakness issues once youâre closer to your ideal weight. ME: (blank stareâ¦flashbacks to every shithead man Iâve ever datedâ¦) TGS: Iâm going to ask you to restrict yourself to 800 calories a day. The easiest way to do this is for you to focus on some kind of liquid diet â protein, low sugar. We can introduce some fiber after week or so, but Iâd like to ask you to try this approach until you are down at least 15 pounds. Then we can add in some more variety. This isnât a long-term lifestyle change, this is a diagnostic tool. ME: OK? I meanâ¦this goes against everything Iâve worked on in therapy for the past, oh say, 20 years of my life. But I will humor you. I will remove food from my life for a short time period. TGS: Next, Iâm putting you on another course of high dose steroids. ME: Seriously? I mean, youâre kidding right? Did you just tell me you want me to lose 30 pounds in short order AND youâre putting me on steroids?!?!? TGS: We need to see if you respond to the steroids to determine the role inflammation (medical jargon, blah blah blah, possibly progressive scenario, but no panicking yet blah blah blah). ME: You know what? Whatever. Fine. Sign me up. Iâm getting on the crazy train with you, Dr. Scott. You better not fucking let me down. It went on from there. Apparently along with being fat, Iâm also old! I mean, I knew that before today but I didnât know how unusual it is for such a late in life diagnosis to be such a rat bastard to get under control. I wouldnât qualify for most clinical trials based on my age alone. Most aggressive treatment options are less likely to be tested on me because, again, Iâm old! Fat and old. Donât forget the fat part. The bottom line is, I will do the round of steroids. I will drink my damn meals â for the time being. I will have my next Ocrevus infusion on Monday. We will see if I improve (four months, says TGS) and we will meet again in August to determine next steps. I will walk gracefully into his office, no Stanley in sight, svelte and lean and never having felt better in my life (OR looked better) before in my life! And we will laugh about that time we thought I might not get better like it was a bad dream starring some kind of pretty, if moderately chubby, middle-aged woman. Or, I will stumble into the lair of TGS, or better yet wheel in on my awesome new motorized wheel-scooter (because I finally got tired of stumbling all over the damn place) and then The Great Scott and I will discuss giving Lemtrada to a middle-aged woman who shouldnât be getting worse so quickly and who is likely not the best, most ideal candidate for Lemtrada in the first place. But we will discuss it. Because weâre going to figure this thing out. PS . I know there will be a lot of people with opinions, very strong ones, about how one should go about losing weight and Iâm almost positive most of them wonât resemble going on a liquid diet for any period of time. But please understand, I know those arguments. ALL of them. I promise you. I appreciate your good intentions, I really do, but Iâll be better off if you keep that advice to yourself just now. Thanks in advance. It means the world to me.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# printing some random reviews\n",
    "sent_0 = final['Text'].values[0]\n",
    "print(sent_0)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_1000 = final['Text'].values[1000]\n",
    "print(sent_1000)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_1500 = final['Text'].values[1500]\n",
    "print(sent_1500)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_4900 = final['Text'].values[4900]\n",
    "print(sent_4900)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 (previously stable on natalizumab), with 55% switching to fingolimod\n"
     ]
    }
   ],
   "source": [
    "# remove urls from text python: https://stackoverflow.com/a/40823105/4084039\n",
    "sent_0 = re.sub(r\"http\\S+\", \"\", sent_0)\n",
    "sent_1000 = re.sub(r\"http\\S+\", \"\", sent_1000)\n",
    "sent_150 = re.sub(r\"http\\S+\", \"\", sent_1500)\n",
    "print(sent_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply posted for JessZidek. Hi Jess Sorry to read about the challenges you are having with your health. You mentioned a lot in your post. I just want to share some info on a few of the points. First, I know you said that you are scared of Humira. Humira and other biologics are very successful in reducing symptoms and inducing and maintain disease remission. To reduce your level of fear it can help to learn more about your treatment option. You can learn more about some of your treatment options. To learn more view our Understanding IBD Medication brochure at:  . If you would like to talk, contact the Help Center at 888-694-8872 or at info@crohnscolitisfoundation.org\n"
     ]
    }
   ],
   "source": [
    "# remove urls from text python: https://stackoverflow.com/a/40823105/4084039\n",
    "sent_0 = re.sub(r\"http\\S+\", \"\", sent_0)\n",
    "sent_1000 = re.sub(r\"http\\S+\", \"\", sent_1000)\n",
    "sent_150 = re.sub(r\"http\\S+\", \"\", sent_1500)\n",
    "sent_4900 = re.sub(r\"http\\S+\", \"\", sent_4900)\n",
    "\n",
    "print(sent_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 (previously stable on natalizumab), with 55% switching to fingolimod\n",
      "==================================================\n",
      "Hi Folks, Can anyone share the experiences with combo yervoy + Keytruda ? My mom is starting her first infusion next week. I am really scared and making false wishes that I can escape this but the truth is I have to walk this path with mom to her recovery. She is so healthy I am afraid this will put her down. Please share experiences.\n",
      "==================================================\n",
      "@fxms , Iâm not sure what more Roche can do. Itâs now becoming a bit of a âbuggerâs muddleâ. If NICE had agreed a differential pricing for PPMS, how would that be managed? And, would there be a temptation to âcook the booksââ¦â¦. Surely no\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(sent_0, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup = BeautifulSoup(sent_1000, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup = BeautifulSoup(sent_1500, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "#soup = BeautifulSoup(sent_4900, 'lxml')\n",
    "#text = soup.get_text()\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply posted for JessZidek. Hi Jess Sorry to read about the challenges you are having with your health. You mentioned a lot in your post. I just want to share some info on a few of the points. First, I know you said that you are scared of Humira. Humira and other biologics are very successful in reducing symptoms and inducing and maintain disease remission. To reduce your level of fear it can help to learn more about your treatment option. You can learn more about some of your treatment options. To learn more view our Understanding IBD Medication brochure at:  . If you would like to talk, contact the Help Center at 888-694-8872 or at info@crohnscolitisfoundation.org\n",
      "==================================================\n",
      "I have my 2nd Entyvio infusion the day after tomorrow. Have had a tiny little setback -- 3 days after my 1st infusion, I developed a bladder infection, so was put on cipro for a week, which I just finished yesterday. Have been having a little bit of stomach pains -- nothing severe, but enough to let me know that this flare is not over yet. Mostly doing fine, still only 1-2 BMs a day, formed. A trace of blood here and there, and very mild cramping. Definitely could be the Cipro. Sat. night, I felt really rough with upper abdominal pain, more heartburn-ish, but I was also bad that day and had some chips and nacho cheese and then some apple pie with ice cream for dessert. (it's so hard to resist some times!!!) Have been using Canasa suppositories every night. Waiting on my Uceris foam, which I got the okay on from Salix (no cost to me!!!), and they are delivering that on Wed. Hoping to start weaning off Prednisone when that comes. I went down to 25 mg, but haven't been able to go lower than that yet. Also, been *really* tired the last few days. Have had to take some naps, and I never, ever nap. This weekend, I was so tired, I was just draggy, and didn't have the energy to do anything except sit. Also been having a lot of headaches, which have not been my typical right-sided migraine. These were more forehead/sinus/top of the head, and my migraine meds didn't touch them. Wondering if the tiredness and headaches are related to Entyvio. I expected that to only last a couple of days after the infusion, not a prolonged reaction. So, we'll see what happens after this next one. Wondering if the UTI was related to Entyvio infusion/lower immunity, or just coincidence. Hoping just coincidental. Partial Colectomy for diverticulitis Sept 2014 UC Diagnosis March 2016 - 18 days in hospital with pancolitis Apriso .375 g x 4/day Canasa suppositories as needed Methotrexate 2.5 mg x 4/week Levothyroxine .50 mcg/day Zoloft for depression Supplements: Zinc, L-glutamine, VSL #3, Folic Acid, CoQ10, turmeric, Started Entyvio Wed., 3/29/17 Constant flares since March 2016 only relieved by Prednisone\n",
      "==================================================\n",
      "Thank you, Merry. You are a world of knowledge and support. I actually joined a Chemo Support group on FB and everyone shares knowledge and experience. I did get a wig a couple of weeks ago and ordered a couple of the Chemo caps, which look horrible on me! Guess I can wear them around the house. I need more poof. Have read it is better to buzz your head rather then shave it. Yes, this is very hard as I am one that curls and styles my hair every day. Now, I haven't washed it for a week. I guess, since I have not been given many options for treatment, this almost seems futile but I do not want to die any time soon. I would talk to Wakekee before I make any decisions. Was thinking of maybe going to Scripps but, again, would be difficult and I hate to be far from home. Will see what he says this morning.  Jump to this post Saw my dictir this morning. My Neutrophols are only .8! He us skipping Chemo this week, will recheck blood work on Tuesday, Chemo next week for 3 days the 5 days of Granix injections. Going to try and schedule me to get a port put in (shudder) then a PET scan. I was going to ask him how he thought I was doing but guess he can't answer that until after the scan. Asked him about the pains I am getting and he didn't know why. Had pain in my cgest and by my left arm pit this morning. Hard to believe he has done this fir so many years and has no answers. To,d him I found a couple of posts where people with this LCNELC are also getting Keytruda or Opvido but he says that is immunotherapy and he and Wakelee are wary of trying that on me because, this undiagnosed, they are sure I have some immune probkems going on since before this cancer diagnosis. Guess it is a wait and see. Wish he were more talkative.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today was awesome. The Great Scott solved all of my problems. We have a great treatment plan devised that is sure to bring me to new levels of health and wellness. I went in hopeless and I re-emerged energized! Full of hope. MS does NOT have me! Okâ¦Iâm just fucking with you. Thatâs not how today went at all. Today wasâ¦a lot. Iâm going to try and break it down in such a way that wonât result in The Great Scott getting hate mail. I must stress up front, that I trust this man. I know he wants whatâs best for me. Iâm in his category of âdifficultâ cases and he seems dedicated to working with me to get me to some kind of stability. While I do not doubt his intentions for even a single second, nor do I doubt his knowledge or skill or experience, I do wish heâd work on his words a bit. His non-medical jargon words are not terribly helpful. His words areâ¦just off. The thing is, if he did that, if he got better with his words then these posts wouldnât be so entertaining, so in a way heâs helping me even when heâs not helping me! Right? Not exactly. I was twenty minutes late to my appointment because of the construction going on in the parking garage at TGSâs office. I had to walk far (very far) to get to his office. By the time I got there, 20 minutes late, I could barely stand up. He was very gracious about my late arrival â wasnât perturbed at all. He was delighted to see me â but noted Stanley right off the bat. âSo, things havenât gotten any better, it would appear? Letâs see you walk. Come with me,â and he guided me into the hallway. I walked about 20 feet, slowly and stumbling. âNow letâs get you out into the hallway.â I looked at him like Iâd kill him dead with my own two hands, or with Stanley, but what choice did I have? Off we went. He measured it out in advance, I suppose, heâs clearly done this bit with patients before. He tskâed and nodded while walking just slightly behind me. Then we got back to the exam room and he asked me to grab Stanley and weâd do it all again! Hooray! Sweet baby Jesus Christ on a cracker. By this time, I am openly stumbling. But my speed does improve a tad with Stanley and TGS seems pleased. Then the words started. The talking began, again. TGS: So the inactivity has led to some weight gain I can see it. How much weight have you gained, Maribeth? ME: Umâ¦I have no Idea. I havenât weighed myself in 20 years. TGS: Until now â get out here, Iâm weighing you right now. Thatâs ridiculous. You are not a ridiculous person, Maribeth. (He is very wrong about this but onward)  ME: Ok, fine, but do NOT tell me what the scale says. Trust me on this. I have major body image, weight-related issues and you might think I can handle this but trust me when I tell you I probably cannot. TGS: Ok, Maribeth weâll do it your way but youâre getting on the scale. This might be a good time to explain why I havenât weighed myself in over 20 years. I have been in therapy for a very long time. The reasons are diverse and vast, no doubt, but one of the biggest reasons is that Iâve had lifelong issues with body image and weight. To make a long, painful & a little bit ridiculous history short, suffice to say that Iâd somehow gotten it into my head that I was too pretty to be overweight. While I was never âfatâ per se, I was also never thin. I remember being told by my long-term college boyfriend how Iâd be âthe prettiest girl on campusâ if I lost 10 pounds. Then later in life my mother-in-law was openly disdainful of my less than skinny figure. She thought her son deserved better. Her son didnât agree with her, but this didnât really help me out so much. That womanâs disdain scarred me. Throughout my teens, young adulthood and even into my 40s I was obsessed with the idea that the worst thing I could ever be was to be fat. Another boyfriend, this one sobbed and sobbed when I finally broke up with him, he found an old picture of me from a few years prior while on a trip with a friend to Key West and he held it up in front of me and said, âSee? You CAN be thin, you just donât want to be thin bad enough.â Suffice to say, he wasnât my boyfriend for long after that but he was my boyfriend for longer than he should have been. Itâs a testament to my poor self-esteem that I didnât kick him out on the spot. The truth is, I thought he was right. I was ashamed of my inability to be skinny. He also had no idea of the extremes I had gone to throughout my life to starve myself into submission. Drugs were involved. Scary habits were involved. Without spilling all of my dirty laundry here on the internets, you can rest assured that I went to every single extreme you are imagining right now in my pursuit of thinness, and probably a few extreme measures you havenât thought of yet. I was too pretty to be fat. You canât be pretty AND fat. If I wasnât pretty, pleasing and attractive to men I was worthless. No matter how successful I was or funny or smart there would always be that one thing I could never seem to be and that was thin. Or thin enough? Iâve been in therapy for 20 years folks. I know my issues inside and out and this notion that I am somehow unacceptable for not being perfectly svelte is at the root of very many of my many other issues. Itâs like the Mack Daddy of my issues. The OG issue. The original vampire of issues that sucked the joy out of my life like so much young, innocent blood for most if not all of my life. Iâm 51 years old now. Iâve worked long and hard to put my fear of fat in its place. Iâm never completely over it, but Iâm so much better now than Iâve ever been. Iâm carrying some extra pounds right now. I know this because of my clothes and how they fit, or donât fit, as the case may be. Iâve told myself over and over again that I need to stop falling into old habits and allowing myself to feel inferior or somehow less acceptable because Iâm carrying a few extra pounds. Cheryl, my therapist, at one point in my mid-30s forbade me to even utter the word âdiet.â Focus on diet isnât good for my mental health (or my physical health). Iâve spent many years working this out. I was a little bigger than I wanted to be, sure, but I was also healthy. So, fuck society and the annoying expectations of most (white) men Iâve had the pleasure of being in relationships with. I was OK with me just the way I was. Kind of. Sometimes. I work hard at it and I mostly fail, but I keep trying because I think itâs important so I am mostly ok. Until Iâm not. TGS: Maribeth, we have to take some novel approaches to try to figure out whatâs going on with you. The first thing Iâm going to ask you to do is to lose 25 to 30 pounds quickly. I know that sounds extreme, but we need to see how you do with your walking and weakness issues once youâre closer to your ideal weight. ME: (blank stareâ¦flashbacks to every shithead man Iâve ever datedâ¦) TGS: Iâm going to ask you to restrict yourself to 800 calories a day. The easiest way to do this is for you to focus on some kind of liquid diet â protein, low sugar. We can introduce some fiber after week or so, but Iâd like to ask you to try this approach until you are down at least 15 pounds. Then we can add in some more variety. This isnât a long-term lifestyle change, this is a diagnostic tool. ME: OK? I meanâ¦this goes against everything Iâve worked on in therapy for the past, oh say, 20 years of my life. But I will humor you. I will remove food from my life for a short time period. TGS: Next, Iâm putting you on another course of high dose steroids. ME: Seriously? I mean, youâre kidding right? Did you just tell me you want me to lose 30 pounds in short order AND youâre putting me on steroids?!?!? TGS: We need to see if you respond to the steroids to determine the role inflammation (medical jargon, blah blah blah, possibly progressive scenario, but no panicking yet blah blah blah). ME: You know what? Whatever. Fine. Sign me up. Iâm getting on the crazy train with you, Dr. Scott. You better not fucking let me down. It went on from there. Apparently along with being fat, Iâm also old! I mean, I knew that before today but I didnât know how unusual it is for such a late in life diagnosis to be such a rat bastard to get under control. I wouldnât qualify for most clinical trials based on my age alone. Most aggressive treatment options are less likely to be tested on me because, again, Iâm old! Fat and old. Donât forget the fat part. The bottom line is, I will do the round of steroids. I will drink my damn meals â for the time being. I will have my next Ocrevus infusion on Monday. We will see if I improve (four months, says TGS) and we will meet again in August to determine next steps. I will walk gracefully into his office, no Stanley in sight, svelte and lean and never having felt better in my life (OR looked better) before in my life! And we will laugh about that time we thought I might not get better like it was a bad dream starring some kind of pretty, if moderately chubby, middle-aged woman. Or, I will stumble into the lair of TGS, or better yet wheel in on my awesome new motorized wheel-scooter (because I finally got tired of stumbling all over the damn place) and then The Great Scott and I will discuss giving Lemtrada to a middle-aged woman who shouldnât be getting worse so quickly and who is likely not the best, most ideal candidate for Lemtrada in the first place. But we will discuss it. Because weâre going to figure this thing out. PS . I know there will be a lot of people with opinions, very strong ones, about how one should go about losing weight and Iâm almost positive most of them wonât resemble going on a liquid diet for any period of time. But please understand, I know those arguments. ALL of them. I promise you. I appreciate your good intentions, I really do, but Iâll be better off if you keep that advice to yourself just now. Thanks in advance. It means the world to me.\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(sent_0, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup = BeautifulSoup(sent_1000, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup = BeautifulSoup(sent_1500, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup = BeautifulSoup(sent_4900, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@fxms , Iâm not sure what more Roche can do. Itâs now becoming a bit of a âbuggerâs muddleâ. If NICE had agreed a differential pricing for PPMS, how would that be managed? And, would there be a temptation to âcook the booksââ¦â¦. Surely no\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "sent_1500 = decontracted(sent_1500)\n",
    "print(sent_1500)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you, Merry. You are a world of knowledge and support. I actually joined a Chemo Support group on FB and everyone shares knowledge and experience. I did get a wig a couple of weeks ago and ordered a couple of the Chemo caps, which look horrible on me! Guess I can wear them around the house. I need more poof. Have read it is better to buzz your head rather then shave it. Yes, this is very hard as I am one that curls and styles my hair every day. Now, I have not washed it for a week. I guess, since I have not been given many options for treatment, this almost seems futile but I do not want to die any time soon. I would talk to Wakekee before I make any decisions. Was thinking of maybe going to Scripps but, again, would be difficult and I hate to be far from home. Will see what he says this morning.  Jump to this post Saw my dictir this morning. My Neutrophols are only .8! He us skipping Chemo this week, will recheck blood work on Tuesday, Chemo next week for 3 days the 5 days of Granix injections. Going to try and schedule me to get a port put in (shudder) then a PET scan. I was going to ask him how he thought I was doing but guess he can not answer that until after the scan. Asked him about the pains I am getting and he did not know why. Had pain in my cgest and by my left arm pit this morning. Hard to believe he has done this fir so many years and has no answers. To,d him I found a couple of posts where people with this LCNELC are also getting Keytruda or Opvido but he says that is immunotherapy and he and Wakelee are wary of trying that on me because, this undiagnosed, they are sure I have some immune probkems going on since before this cancer diagnosis. Guess it is a wait and see. Wish he were more talkative.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "sent_1500 = decontracted(sent_1500)\n",
    "print(sent_1500)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(previously stable on natalizumab), with  switching to fingolimod\n"
     ]
    }
   ],
   "source": [
    "#remove words with numbers python: https://stackoverflow.com/a/18082370/4084039\n",
    "sent_0 = re.sub(\"\\S*\\d\\S*\", \"\", sent_0).strip()\n",
    "print(sent_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply posted for JessZidek. Hi Jess Sorry to read about the challenges you are having with your health. You mentioned a lot in your post. I just want to share some info on a few of the points. First, I know you said that you are scared of Humira. Humira and other biologics are very successful in reducing symptoms and inducing and maintain disease remission. To reduce your level of fear it can help to learn more about your treatment option. You can learn more about some of your treatment options. To learn more view our Understanding IBD Medication brochure at:  . If you would like to talk, contact the Help Center at  or at info@crohnscolitisfoundation.org\n"
     ]
    }
   ],
   "source": [
    "#remove words with numbers python: https://stackoverflow.com/a/18082370/4084039\n",
    "sent_0 = re.sub(\"\\S*\\d\\S*\", \"\", sent_0).strip()\n",
    "print(sent_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fxms I m not sure what more Roche can do It s now becoming a bit of a bugger s muddle If NICE had agreed a differential pricing for PPMS how would that be managed And would there be a temptation to cook the books Surely no\n"
     ]
    }
   ],
   "source": [
    "#remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
    "sent_1500 = re.sub('[^A-Za-z0-9]+', ' ', sent_1500)\n",
    "print(sent_1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you Merry You are a world of knowledge and support I actually joined a Chemo Support group on FB and everyone shares knowledge and experience I did get a wig a couple of weeks ago and ordered a couple of the Chemo caps which look horrible on me Guess I can wear them around the house I need more poof Have read it is better to buzz your head rather then shave it Yes this is very hard as I am one that curls and styles my hair every day Now I have not washed it for a week I guess since I have not been given many options for treatment this almost seems futile but I do not want to die any time soon I would talk to Wakekee before I make any decisions Was thinking of maybe going to Scripps but again would be difficult and I hate to be far from home Will see what he says this morning Jump to this post Saw my dictir this morning My Neutrophols are only 8 He us skipping Chemo this week will recheck blood work on Tuesday Chemo next week for 3 days the 5 days of Granix injections Going to try and schedule me to get a port put in shudder then a PET scan I was going to ask him how he thought I was doing but guess he can not answer that until after the scan Asked him about the pains I am getting and he did not know why Had pain in my cgest and by my left arm pit this morning Hard to believe he has done this fir so many years and has no answers To d him I found a couple of posts where people with this LCNELC are also getting Keytruda or Opvido but he says that is immunotherapy and he and Wakelee are wary of trying that on me because this undiagnosed they are sure I have some immune probkems going on since before this cancer diagnosis Guess it is a wait and see Wish he were more talkative \n"
     ]
    }
   ],
   "source": [
    "#remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
    "sent_1500 = re.sub('[^A-Za-z0-9]+', ' ', sent_1500)\n",
    "print(sent_1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "# <br /><br /> ==> after the above steps, we are getting \"br br\"\n",
    "# we are including them into stop words list\n",
    "# instead of <br /> if we have <br/> these tags would have revmoved in the 1st step\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2924/2924 [00:04<00:00, 668.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_reviews1 = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(test_df['text'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_reviews1.append(sentance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5245/5245 [00:06<00:00, 775.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_reviews = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(df2['text'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_reviews.append(sentance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Preprocessed Reviews is the cleaned text data  - Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reply posted for JessZidek. Hi Jess Sorry to r...</td>\n",
       "      <td>reply posted jesszidek hi jess sorry read chal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last Updated: January 01, 2017.  Share | Comme...</td>\n",
       "      <td>last updated january share comments tell frien...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi I was on rebif for about a year â rotate ...</td>\n",
       "      <td>hi rebif year rotate injection sites remember ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No problem. I know how hard and lonely this jo...</td>\n",
       "      <td>no problem know hard lonely journey happy help...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conclusion: These real-life results suggest th...</td>\n",
       "      <td>conclusion real life results suggest intravitr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Reply posted for JessZidek. Hi Jess Sorry to r...   \n",
       "1  Last Updated: January 01, 2017.  Share | Comme...   \n",
       "2  Hi I was on rebif for about a year â rotate ...   \n",
       "3  No problem. I know how hard and lonely this jo...   \n",
       "4  Conclusion: These real-life results suggest th...   \n",
       "\n",
       "                                        Cleaned Text sentiment  \n",
       "0  reply posted jesszidek hi jess sorry read chal...         0  \n",
       "1  last updated january share comments tell frien...         0  \n",
       "2  hi rebif year rotate injection sites remember ...         0  \n",
       "3  no problem know hard lonely journey happy help...         0  \n",
       "4  conclusion real life results suggest intravitr...         0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Cleaned Text'] = preprocessed_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>be5a13376933a7f9bbf8e801c31691092f63260a</td>\n",
       "      <td>Reply posted for JessZidek. Hi Jess Sorry to r...</td>\n",
       "      <td>humira</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e8f39b50683bb1b94689e8e462bdcd1aff331ee1</td>\n",
       "      <td>Last Updated: January 01, 2017.  Share | Comme...</td>\n",
       "      <td>ocrelizumab</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c2df2a0e70805bb1a17305e2ac137aeae26d424a</td>\n",
       "      <td>Hi I was on rebif for about a year â rotate ...</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>603c2f1612eeabcaac016b6da0df4117b6a8ccd8</td>\n",
       "      <td>No problem. I know how hard and lonely this jo...</td>\n",
       "      <td>tecentriq</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  be5a13376933a7f9bbf8e801c31691092f63260a   \n",
       "1  e8f39b50683bb1b94689e8e462bdcd1aff331ee1   \n",
       "2  c2df2a0e70805bb1a17305e2ac137aeae26d424a   \n",
       "3  603c2f1612eeabcaac016b6da0df4117b6a8ccd8   \n",
       "\n",
       "                                                text         drug sentiment  \n",
       "0  Reply posted for JessZidek. Hi Jess Sorry to r...       humira         0  \n",
       "1  Last Updated: January 01, 2017.  Share | Comme...  ocrelizumab         0  \n",
       "2  Hi I was on rebif for about a year â rotate ...   fingolimod         0  \n",
       "3  No problem. I know how hard and lonely this jo...    tecentriq         0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['sentiment'] = df2['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new123 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new123['Cleaned Text'] = final['Cleaned Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reply posted jesszidek hi jess sorry read chal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>last updated january share comments tell frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi rebif year rotate injection sites remember ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem know hard lonely journey happy help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conclusion real life results suggest intravitr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Cleaned Text\n",
       "0  reply posted jesszidek hi jess sorry read chal...\n",
       "1  last updated january share comments tell frien...\n",
       "2  hi rebif year rotate injection sites remember ...\n",
       "3  no problem know hard lonely journey happy help...\n",
       "4  conclusion real life results suggest intravitr..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new123.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new123['length'] = k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(preprocessed_reviews)):\n",
    "    k1.append(len(preprocessed_reviews[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(new123,final['sentiment'].values,test_size=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3671,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1574,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3671, 2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1574, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3671"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reply posted jesszidek hi jess sorry read chal...</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>last updated january share comments tell frien...</td>\n",
       "      <td>14476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi rebif year rotate injection sites remember ...</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem know hard lonely journey happy help...</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conclusion real life results suggest intravitr...</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Cleaned Text  length\n",
       "0  reply posted jesszidek hi jess sorry read chal...     401\n",
       "1  last updated january share comments tell frien...   14476\n",
       "2  hi rebif year rotate injection sites remember ...     162\n",
       "3  no problem know hard lonely journey happy help...     785\n",
       "4  conclusion real life results suggest intravitr...     134"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>beth second ocrevus infusion yesterday nurses ...</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3672</th>\n",
       "      <td>diagnosed uc years ago total colectomy surgery...</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>rilonacept used treat symptoms rare genetic co...</td>\n",
       "      <td>5279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3674</th>\n",
       "      <td>max dose lialda tablets day started flaring in...</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>grandma think may find tad daunting quite hone...</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Cleaned Text  length\n",
       "3671  beth second ocrevus infusion yesterday nurses ...     326\n",
       "3672  diagnosed uc years ago total colectomy surgery...     624\n",
       "3673  rilonacept used treat symptoms rare genetic co...    5279\n",
       "3674  max dose lialda tablets day started flaring in...    1100\n",
       "3675  grandma think may find tad daunting quite hone...     170"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainbow = pd.DataFrame()\n",
    "X_trainbow['Cleaned Text'] = X_train1['Cleaned Text']\n",
    "X_trainbow['Length'] = X_train1['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testbow = pd.DataFrame()\n",
    "X_testbow['Cleaned Text'] = X_test1['Cleaned Text']\n",
    "X_testbow['Length'] = X_test1['length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING BAG OF WORDS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() \n",
    "a1 = count_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "b1 = count_vect.transform(X_testbow['Cleaned Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer :  (3671, 27066)\n",
      "the number of unique words : 27066\n"
     ]
    }
   ],
   "source": [
    "print(\"the type of count vectorizer :\",type(a1))\n",
    "print(\"the shape of out text BOW vectorizer : \",a1.get_shape())\n",
    "print(\"the number of unique words :\", a1.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import *\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = preprocessing.normalize(a1)\n",
    "b1 = preprocessing.normalize(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = sparse.csr_matrix(X_train1['length'].values)\n",
    "a2 = preprocessing.normalize(a2)\n",
    "a3 = sparse.hstack([a1, a2.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = sparse.csr_matrix(X_test1['length'].values)\n",
    "b2 = preprocessing.normalize(b2)\n",
    "b3 = sparse.hstack([b1, b2.T])                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING F1 MICRO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-95148f61c87a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Using GridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'multinomial'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuned_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'f1_micro'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    664\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 666\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1604\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1605\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1606\u001b[1;33m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1608\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    942\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 944\u001b[1;33m                 iprint=iprint, pgtol=tol, maxiter=max_iter)\n\u001b[0m\u001b[0;32m    945\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"warnflag\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m                 warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[1;32m--> 199\u001b[1;33m                            **opts)\n\u001b[0m\u001b[0;32m    200\u001b[0m     d = {'grad': res['jac'],\n\u001b[0;32m    201\u001b[0m          \u001b[1;34m'task'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, *args)\u001b[0m\n\u001b[0;32m    917\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_multi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 919\u001b[1;33m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_multinomial_loss_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    920\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss_grad\u001b[1;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m     \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m     \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \"\"\"\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"toarray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__rmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[0mtr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[1;31m#####################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    471\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_mul_multivector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_matvecs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m         fn(M, N, n_vecs, self.indptr, self.indices, self.data,\n\u001b[1;32m--> 482\u001b[1;33m            other.ravel(), result.ravel())\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#refer: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\n",
    "#tuned_parameters = [{'C': [10**2, 10**4,10**5]}]\n",
    "tuned_parameters = [{'C': [10**-5,10**-4, 10**-2, 10**0, 10**2, 10**4,10**5]}]\n",
    "# AS C INCREASES IT SHOULD OVERFIT. HERE C IS NOTHING BUT 1/LAMDA. LAMDA IS THE HYPER PARAMETER.\n",
    "#Using GridSearchCV\n",
    "model1 = GridSearchCV(LogisticRegression(solver = 'lbfgs',multi_class='multinomial', max_iter = 1500), tuned_parameters, scoring = 'f1_micro', cv=5, return_train_score= True)\n",
    "model1.fit(a3, y_train1)\n",
    "print(model1.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9161372299872935\n"
     ]
    }
   ],
   "source": [
    "print(model1.score(b3, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'C': [10**-5,10**-4, 10**-2, 10**0, 10**2, 10**4,10**5]}]\n",
    "# AS C INCREASES IT SHOULD OVERFIT. HERE C IS NOTHING BUT 1/LAMDA. LAMDA IS THE HYPER PARAMETER.\n",
    "#Using GridSearchCV\n",
    "model11 = GridSearchCV(LogisticRegression(solver = 'lbfgs',multi_class='multinomial', max_iter = 1500), tuned_parameters, scoring = 'f1_micro', cv=5, return_train_score= True)\n",
    "model11.fit(a1, y_train1)\n",
    "print(model11.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9148665819567979\n"
     ]
    }
   ],
   "source": [
    "print(model11.score(b1, y_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING F1 MACRO - BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2849521037953195\n"
     ]
    }
   ],
   "source": [
    "#tuned_parameters = [{'C': [10**2, 10**4,10**5]}]\n",
    "tuned_parameters = [{'C': [10**-5,10**-4, 10**-2, 10**0, 10**2, 10**4,10**5]}]\n",
    "# AS C INCREASES IT SHOULD OVERFIT. HERE C IS NOTHING BUT 1/LAMDA. LAMDA IS THE HYPER PARAMETER.\n",
    "#Using GridSearchCV\n",
    "model2 = GridSearchCV(LogisticRegression(solver = 'lbfgs',multi_class='multinomial', max_iter = 1500), tuned_parameters, scoring = 'f1_macro', cv=5, return_train_score= True)\n",
    "model2.fit(a3, y_train1)\n",
    "print(model2.best_estimator_)\n",
    "print(model2.score(b3, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (3671, 27066)\n",
      "the number of unique words : 27066\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer()\n",
    "c1 = tf_idf_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "d1 = tf_idf_vect.transform(X_testbow['Cleaned Text'])\n",
    "print(\"the type of count vectorizer :\",type(c1))\n",
    "print(\"the shape of out text TFIDF vectorizer : \",c1.get_shape())\n",
    "print(\"the number of unique words :\", c1.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = preprocessing.normalize(c1)\n",
    "c2 = sparse.csr_matrix(X_train1['length'].values)\n",
    "c2 = preprocessing.normalize(c2)\n",
    "c3 = sparse.hstack([c1, c2.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = preprocessing.normalize(d1)\n",
    "d2 = sparse.csr_matrix(X_test1['length'].values)\n",
    "d2 = preprocessing.normalize(d2)\n",
    "d3 = sparse.hstack([d1, d2.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF - F1 MICRO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.9275730622617535\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'C': [10**-5,10**-4, 10**-2, 10**0, 10**2, 10**4,10**5]}]\n",
    "# AS C INCREASES IT SHOULD OVERFIT. HERE C IS NOTHING BUT 1/LAMDA. LAMDA IS THE HYPER PARAMETER.\n",
    "#Using GridSearchCV\n",
    "model3 = GridSearchCV(LogisticRegression(solver = 'lbfgs',multi_class='multinomial', max_iter = 1500), tuned_parameters, scoring = 'f1_micro', cv=5, return_train_score= True)\n",
    "model3.fit(c3, y_train1)\n",
    "print(model3.best_estimator_)\n",
    "print(model3.score(d3, y_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF - F1 MACRO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28467419002548233\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'C': [10**-5,10**-4, 10**-2, 10**0, 10**2, 10**4,10**5]}]\n",
    "# AS C INCREASES IT SHOULD OVERFIT. HERE C IS NOTHING BUT 1/LAMDA. LAMDA IS THE HYPER PARAMETER.\n",
    "#Using GridSearchCV\n",
    "model4 = GridSearchCV(LogisticRegression(solver = 'lbfgs',multi_class='multinomial', max_iter = 1500), tuned_parameters, scoring = 'f1_macro', cv=5, return_train_score= True)\n",
    "model4.fit(c3, y_train1)\n",
    "print(model4.best_estimator_)\n",
    "print(model4.score(d3, y_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVGW2V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "list_of_sentance=[]\n",
    "for sentance in (X_trainbow['Cleaned Text'].values):\n",
    "    list_of_sentance.append(sentance.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hear', 0.9803595542907715), ('sorry', 0.9768660068511963), ('hope', 0.9612845778465271), ('thoughts', 0.9600058794021606), ('glad', 0.9597508907318115), ('luck', 0.9572322368621826), ('everyone', 0.9561948776245117), ('happy', 0.9555653929710388), ('love', 0.9507182240486145), ('thank', 0.9481939673423767)]\n",
      "==================================================\n",
      "[('awful', 0.984891951084137), ('horrible', 0.974980354309082), ('weird', 0.9741758108139038), ('relief', 0.9739112854003906), ('fun', 0.9711474180221558), ('helped', 0.9701564311981201), ('lady', 0.9699545502662659), ('noticed', 0.9691743850708008), ('gotten', 0.9666872024536133), ('hell', 0.9662218689918518)]\n",
      "number of words that occured minimum 5 times  9013\n",
      "sample words  ['reply', 'posted', 'hi', 'sorry', 'read', 'challenges', 'health', 'mentioned', 'lot', 'post', 'want', 'share', 'info', 'points', 'first', 'know', 'said', 'scared', 'humira', 'biologics', 'successful', 'reducing', 'symptoms', 'inducing', 'maintain', 'disease', 'remission', 'reduce', 'level', 'fear', 'help', 'learn', 'treatment', 'option', 'options', 'view', 'understanding', 'ibd', 'medication', 'brochure', 'would', 'like', 'talk', 'contact', 'center', 'org', 'last', 'updated', 'january', 'comments']\n"
     ]
    }
   ],
   "source": [
    "#WORD TO VECTOR\n",
    "\n",
    "is_your_ram_gt_16g=False\n",
    "want_to_use_google_w2v = False\n",
    "want_to_train_w2v = True\n",
    "\n",
    "if want_to_train_w2v:\n",
    "    # min_count = 5 considers only words that occured atleast 5 times\n",
    "    w2v_model=Word2Vec(list_of_sentance,min_count=5,size=50, workers=4)\n",
    "    print(w2v_model.wv.most_similar('great'))\n",
    "    print('='*50)\n",
    "    print(w2v_model.wv.most_similar('worst'))\n",
    "    \n",
    "elif want_to_use_google_w2v and is_your_ram_gt_16g:\n",
    "    if os.path.isfile('GoogleNews-vectors-negative300.bin'):\n",
    "        w2v_model=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        print(w2v_model.wv.most_similar('great'))\n",
    "        print(w2v_model.wv.most_similar('worst'))\n",
    "    else:\n",
    "        print(\"you don't have gogole's word2vec file, keep want_to_train_w2v = True, to train your own w2v \")\n",
    "\n",
    "\n",
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "print(\"sample words \", w2v_words[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3671/3671 [00:17<00:00, 208.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3671\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in tqdm(list_of_sentance): # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "list_of_sentance1=[]\n",
    "for sentance in (X_test1['Cleaned Text'].values):\n",
    "    list_of_sentance1.append(sentance.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pretty', 0.9978894591331482), ('mind', 0.9973368048667908), ('little', 0.9971143007278442), ('wanted', 0.9968970417976379), ('bit', 0.9967705011367798), ('felt', 0.99553382396698), ('hard', 0.9955053329467773), ('trying', 0.9954373836517334), ('stay', 0.9950129985809326), ('definitely', 0.9945928454399109)]\n",
      "==================================================\n",
      "[('everything', 0.998406171798706), ('ok', 0.9979478120803833), ('nothing', 0.9974957704544067), ('morning', 0.9973071217536926), ('happy', 0.9972931146621704), ('woke', 0.9970003366470337), ('definitely', 0.996965229511261), ('appointments', 0.996893584728241), ('anymore', 0.996607780456543), ('goes', 0.9963794946670532)]\n",
      "number of words that occured minimum 5 times  6336\n",
      "sample words  ['beth', 'second', 'ocrevus', 'infusion', 'yesterday', 'nurses', 'center', 'asking', 'symptoms', 'prior', 'coming', 'apparently', 'lot', 'patients', 'saying', 'notice', 'increase', 'fatigue', 'goes', 'month', 'two', 'scheduled', 'like', 'many', 'past', 'appearing', 'frequent', 'rate', 'months', 'adds', 'mystery', 'ms', 'wait', 'get', 'next', 'mri', 'see', 'way', 'feel', 'day', 'mean', 'disease', 'progressing', 'diagnosed', 'uc', 'years', 'ago', 'total', 'colectomy', 'surgery']\n"
     ]
    }
   ],
   "source": [
    "is_your_ram_gt_16g=False\n",
    "want_to_use_google_w2v = False\n",
    "want_to_train_w2v = True\n",
    "\n",
    "if want_to_train_w2v:\n",
    "    # min_count = 5 considers only words that occured atleast 5 times\n",
    "    w2v_model1=Word2Vec(list_of_sentance1,min_count=5,size=50, workers=4)\n",
    "    print(w2v_model1.wv.most_similar('great'))\n",
    "    print('='*50)\n",
    "    print(w2v_model1.wv.most_similar('worst'))\n",
    "    \n",
    "elif want_to_use_google_w2v and is_your_ram_gt_16g:\n",
    "    if os.path.isfile('GoogleNews-vectors-negative300.bin'):\n",
    "        w2v_model1=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        print(w2v_model1.wv.most_similar('great'))\n",
    "        print(w2v_model1.wv.most_similar('worst'))\n",
    "    else:\n",
    "        print(\"you don't have gogole's word2vec file, keep want_to_train_w2v = True, to train your own w2v \")\n",
    "\n",
    "\n",
    "w2v_words1 = list(w2v_model1.wv.vocab)\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_words1))\n",
    "print(\"sample words \", w2v_words1[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1574/1574 [00:06<00:00, 227.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1574\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "sent_vectors1 = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in tqdm(list_of_sentance1): # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words1:\n",
    "            vec = w2v_model1.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    sent_vectors1.append(sent_vec)\n",
    "print(len(sent_vectors1))\n",
    "print(len(sent_vectors1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3 = sent_vectors\n",
    "f3 = sent_vectors1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3 = preprocessing.normalize(e3)\n",
    "e4 = sparse.csr_matrix(X_train1['length'].values)\n",
    "e4 = preprocessing.normalize(e4)\n",
    "e5 = sparse.hstack([e3, e4.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = preprocessing.normalize(f3)\n",
    "f4 = sparse.csr_matrix(X_test1['length'].values)\n",
    "f4 = preprocessing.normalize(f4)\n",
    "f5 = sparse.hstack([f3, f4.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008460236886632825\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'C': [10**-5,10**-4, 10**-2, 10**0, 10**2, 10**4,10**5]}]\n",
    "# AS C INCREASES IT SHOULD OVERFIT. HERE C IS NOTHING BUT 1/LAMDA. LAMDA IS THE HYPER PARAMETER.\n",
    "#Using GridSearchCV\n",
    "model6 = GridSearchCV(LogisticRegression(solver = 'lbfgs',multi_class='multinomial', max_iter = 1500), tuned_parameters, scoring = 'f1_macro', cv=5, return_train_score= True)\n",
    "model6.fit(e5, y_train1)\n",
    "print(model6.best_estimator_)\n",
    "print(model6.score(f5, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v is not working properly ...\n",
    "# let's move to avg w2v tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF - AVG W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TfidfVectorizer()\n",
    "tf_idf_matrix = model.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3671/3671 [02:02<00:00, 40.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = model.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in tqdm(list_of_sentance): # for each review/sentence \n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model.wv[word]\n",
    "            #tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n",
    "            # to reduce the computation we are \n",
    "            # dictionary[word] = idf value of word in whole courpus\n",
    "            # sent.count(word) = tf valeus of word in this review\n",
    "            tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1574/1574 [00:53<00:00, 29.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = model.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors1 = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in tqdm(list_of_sentance1): # for each review/sentence \n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words1 and word in tfidf_feat:\n",
    "            vec = w2v_model1.wv[word]\n",
    "            #tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n",
    "            # to reduce the computation we are \n",
    "            # dictionary[word] = idf value of word in whole courpus\n",
    "            # sent.count(word) = tf valeus of word in this review\n",
    "            tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors1.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = tfidf_sent_vectors\n",
    "h3 = tfidf_sent_vectors1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = preprocessing.normalize(g3)\n",
    "g4 = sparse.csr_matrix(X_train1['length'].values)\n",
    "g4 = preprocessing.normalize(g4)\n",
    "g5 = sparse.hstack([g3, g4.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = preprocessing.normalize(h3)\n",
    "h4 = sparse.csr_matrix(X_test1['length'].values)\n",
    "h4 = preprocessing.normalize(h4)\n",
    "h5 = sparse.hstack([h3, h4.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016522098306484923\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'C': [10**-5,10**-4, 10**-2, 10**0, 10**2, 10**4,10**5]}]\n",
    "# AS C INCREASES IT SHOULD OVERFIT. HERE C IS NOTHING BUT 1/LAMDA. LAMDA IS THE HYPER PARAMETER.\n",
    "#Using GridSearchCV\n",
    "model6 = GridSearchCV(LogisticRegression(solver = 'lbfgs',multi_class='multinomial', max_iter = 1500), tuned_parameters, scoring = 'f1_macro', cv=5, return_train_score= True)\n",
    "model6.fit(g5, y_train1)\n",
    "print(model6.best_estimator_)\n",
    "print(model6.score(h5, y_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations:\n",
    "1) Only Bow and tf-idf works reasonably better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c3 and d3 should be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the Model with the best hyper parameter c = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before that I am converting the text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1['Cleaned Text'] = preprocessed_reviews1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256 (previously stable on natalizumab), with 5...</td>\n",
       "      <td>previously stable natalizumab switching fingol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On fingolimod and have been since December 201...</td>\n",
       "      <td>fingolimod since december way describe better ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apparently it's shingles! :-/ I do have a few ...</td>\n",
       "      <td>apparently shingles red spots left breast appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the Docetaxel doing once a week x3 weeks th...</td>\n",
       "      <td>docetaxel week weeks week claim less harsh eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC, Stelara worked in a matter of days for me....</td>\n",
       "      <td>cc stelara worked matter days gi willing jump ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  256 (previously stable on natalizumab), with 5...   \n",
       "1  On fingolimod and have been since December 201...   \n",
       "2  Apparently it's shingles! :-/ I do have a few ...   \n",
       "3  If the Docetaxel doing once a week x3 weeks th...   \n",
       "4  CC, Stelara worked in a matter of days for me....   \n",
       "\n",
       "                                        Cleaned Text  \n",
       "0  previously stable natalizumab switching fingol...  \n",
       "1  fingolimod since december way describe better ...  \n",
       "2  apparently shingles red spots left breast appe...  \n",
       "3  docetaxel week weeks week claim less harsh eff...  \n",
       "4  cc stelara worked matter days gi willing jump ...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST - SUBMISSION - TFIDF - L2 PENALTY WITH OUT FEATURE ENGINEERING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (3671, 27066)\n",
      "the number of unique words : 27066\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer()\n",
    "c1 = tf_idf_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "d134 = tf_idf_vect.transform(final1['Cleaned Text'])\n",
    "print(\"the type of count vectorizer :\",type(c1))\n",
    "print(\"the shape of out text TFIDF vectorizer : \",c1.get_shape())\n",
    "print(\"the number of unique words :\", c1.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = preprocessing.normalize(c1)\n",
    "d134 = preprocessing.normalize(d134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_tfidfl1 = LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
    "                   multi_class='multinomial', n_jobs=None, penalty='l1',\n",
    "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "om_tfidfl1.fit(c1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ompredictions_tfidfl1  = om_tfidfl1.predict(d134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub3 = pd.DataFrame()\n",
    "df_sub3['unique_hash'] = test_df['unique_hash']\n",
    "df_sub3['sentiment'] = ompredictions_tfidfl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-1bca046a1de0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_sub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_sub' is not defined"
     ]
    }
   ],
   "source": [
    "#df_sub['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1750\n",
       "1     633\n",
       "0     541\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub2['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1457\n",
       "0     771\n",
       "1     696\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub3['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub3.to_csv('my_submission3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_tfidf = LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
    "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_tfidf1 = LogisticRegression(C=1, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
    "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "om_tfidf.fit(c1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "om_tfidf1.fit(c1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "ompredictions_tfidf  = om_tfidf.predict(d134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "ompredictions_tfidf1  = om_tfidf1.predict(d134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2', '1', '2', ..., '1', '2', '2'], dtype=object)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ompredictions_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub4 = pd.DataFrame()\n",
    "df_sub4['unique_hash'] = test_df['unique_hash']\n",
    "df_sub4['sentiment'] = ompredictions_tfidf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub4.to_csv('my_submission4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e9a8166b84114aca147bf409f6f956635034c08</td>\n",
       "      <td>256 (previously stable on natalizumab), with 5...</td>\n",
       "      <td>fingolimod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e747e6822c867571afe7b907b51f0f2ca67b0e1a</td>\n",
       "      <td>On fingolimod and have been since December 201...</td>\n",
       "      <td>fingolimod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50b6d851bcff4f35afe354937949e9948975adf7</td>\n",
       "      <td>Apparently it's shingles! :-/ I do have a few ...</td>\n",
       "      <td>humira</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae</td>\n",
       "      <td>If the Docetaxel doing once a week x3 weeks th...</td>\n",
       "      <td>tagrisso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8b37d169dee5bdae27060949242fb54feb6a7f7f</td>\n",
       "      <td>CC, Stelara worked in a matter of days for me....</td>\n",
       "      <td>stelara</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  9e9a8166b84114aca147bf409f6f956635034c08   \n",
       "1  e747e6822c867571afe7b907b51f0f2ca67b0e1a   \n",
       "2  50b6d851bcff4f35afe354937949e9948975adf7   \n",
       "3  7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae   \n",
       "4  8b37d169dee5bdae27060949242fb54feb6a7f7f   \n",
       "\n",
       "                                                text        drug  \n",
       "0  256 (previously stable on natalizumab), with 5...  fingolimod  \n",
       "1  On fingolimod and have been since December 201...  fingolimod  \n",
       "2  Apparently it's shingles! :-/ I do have a few ...      humira  \n",
       "3  If the Docetaxel doing once a week x3 weeks th...    tagrisso  \n",
       "4  CC, Stelara worked in a matter of days for me....     stelara  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['unique_hash'] = test_df['unique_hash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['sentiment'] = ompredictions_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2924, 2)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e9a8166b84114aca147bf409f6f956635034c08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e747e6822c867571afe7b907b51f0f2ca67b0e1a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50b6d851bcff4f35afe354937949e9948975adf7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash sentiment\n",
       "0  9e9a8166b84114aca147bf409f6f956635034c08         2\n",
       "1  e747e6822c867571afe7b907b51f0f2ca67b0e1a         1\n",
       "2  50b6d851bcff4f35afe354937949e9948975adf7         2\n",
       "3  7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae         2"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1753\n",
       "1     632\n",
       "0     539\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('my_submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l2 penalty with feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (3671, 27066)\n",
      "the number of unique words : 27066\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer()\n",
    "c1 = tf_idf_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "d134 = tf_idf_vect.transform(final1['Cleaned Text'])\n",
    "print(\"the type of count vectorizer :\",type(c1))\n",
    "print(\"the shape of out text TFIDF vectorizer : \",c1.get_shape())\n",
    "print(\"the number of unique words :\", c1.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "om_tfidf.fit(c3, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "k3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(preprocessed_reviews1)):\n",
    "    k3.append(len(preprocessed_reviews1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256 (previously stable on natalizumab), with 5...</td>\n",
       "      <td>previously stable natalizumab switching fingol...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On fingolimod and have been since December 201...</td>\n",
       "      <td>fingolimod since december way describe better ...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apparently it's shingles! :-/ I do have a few ...</td>\n",
       "      <td>apparently shingles red spots left breast appe...</td>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the Docetaxel doing once a week x3 weeks th...</td>\n",
       "      <td>docetaxel week weeks week claim less harsh eff...</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  256 (previously stable on natalizumab), with 5...   \n",
       "1  On fingolimod and have been since December 201...   \n",
       "2  Apparently it's shingles! :-/ I do have a few ...   \n",
       "3  If the Docetaxel doing once a week x3 weeks th...   \n",
       "\n",
       "                                        Cleaned Text  length  \n",
       "0  previously stable natalizumab switching fingol...      50  \n",
       "1  fingolimod since december way describe better ...      78  \n",
       "2  apparently shingles red spots left breast appe...     647  \n",
       "3  docetaxel week weeks week claim less harsh eff...     528  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2924, 2)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1['length'] = k3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256 (previously stable on natalizumab), with 5...</td>\n",
       "      <td>previously stable natalizumab switching fingol...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On fingolimod and have been since December 201...</td>\n",
       "      <td>fingolimod since december way describe better ...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apparently it's shingles! :-/ I do have a few ...</td>\n",
       "      <td>apparently shingles red spots left breast appe...</td>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  256 (previously stable on natalizumab), with 5...   \n",
       "1  On fingolimod and have been since December 201...   \n",
       "2  Apparently it's shingles! :-/ I do have a few ...   \n",
       "\n",
       "                                        Cleaned Text  length  \n",
       "0  previously stable natalizumab switching fingol...      50  \n",
       "1  fingolimod since december way describe better ...      78  \n",
       "2  apparently shingles red spots left breast appe...     647  "
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "d134 = preprocessing.normalize(d134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "d135 = sparse.csr_matrix(final1['length'].values)\n",
    "d135 = preprocessing.normalize(d135)\n",
    "d136 = sparse.hstack([d134, d135.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "ompredictions_tfidf  = om_tfidf.predict(d136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub2 = pd.DataFrame()\n",
    "df_sub2['unique_hash'] = test_df['unique_hash']\n",
    "df_sub2['sentiment'] = ompredictions_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub2.to_csv('my_submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with out normalising and with out feature engineering TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (3671, 27066)\n",
      "the number of unique words : 27066\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer()\n",
    "c1 = tf_idf_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "d134 = tf_idf_vect.transform(final1['Cleaned Text'])\n",
    "print(\"the type of count vectorizer :\",type(c1))\n",
    "print(\"the shape of out text TFIDF vectorizer : \",c1.get_shape())\n",
    "print(\"the number of unique words :\", c1.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "d134 = preprocessing.normalize(d134)\n",
    "d135 = sparse.csr_matrix(final1['length'].values)\n",
    "d135 = preprocessing.normalize(d135)\n",
    "d136 = sparse.hstack([d134, d135.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_tfidf = LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter= 2000,\n",
    "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=2000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "om_tfidf.fit(c3, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "ompredictions_tfidf  = om_tfidf.predict(d136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub6 = pd.DataFrame()\n",
    "df_sub6['unique_hash'] = test_df['unique_hash']\n",
    "df_sub6['sentiment'] = ompredictions_tfidf\n",
    "df_sub6.to_csv('my_submission6.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() \n",
    "a1 = count_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "b134 = count_vect.transform(final1['Cleaned Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (3671, 27066)\n",
      "the number of unique words : 27066\n"
     ]
    }
   ],
   "source": [
    "#d134 = tf_idf_vect.transform(final1['Cleaned Text'])\n",
    "print(\"the type of count vectorizer :\",type(a1))\n",
    "print(\"the shape of out text TFIDF vectorizer : \",a1.get_shape())\n",
    "print(\"the number of unique words :\", a1.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_bow = LogisticRegression(C=1, class_weight= 'balanced', dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
    "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = preprocessing.normalize(a1)\n",
    "b134 = preprocessing.normalize(b134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "om_bow.fit(a1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "ompredictions_bow  = om_bow.predict(b134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub7 = pd.DataFrame()\n",
    "df_sub7['unique_hash'] = test_df['unique_hash']\n",
    "df_sub7['sentiment'] = ompredictions_bow\n",
    "df_sub7.to_csv('my_submission7.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_score,recall_score\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.0001, class_prior=None, fit_prior=True)\n",
      "0.8780177890724269\n"
     ]
    }
   ],
   "source": [
    "nb=MultinomialNB()\n",
    "parameters={'alpha':[0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\n",
    "model6=GridSearchCV(nb,parameters,cv=25, scoring = 'f1_micro', return_train_score= True)\n",
    "model6.fit(a1, y_train1)\n",
    "print(model6.best_estimator_)\n",
    "print(model6.score(b1, y_test1))\n",
    "#model6.fit(a1, y_train1)\n",
    "#print(model6.best_estimator_)\n",
    "#print(model6.score(b1, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e216348be0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(final['log_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "      <th>log_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256 (previously stable on natalizumab), with 5...</td>\n",
       "      <td>previously stable natalizumab switching fingol...</td>\n",
       "      <td>50</td>\n",
       "      <td>3.912023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On fingolimod and have been since December 201...</td>\n",
       "      <td>fingolimod since december way describe better ...</td>\n",
       "      <td>78</td>\n",
       "      <td>4.356709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apparently it's shingles! :-/ I do have a few ...</td>\n",
       "      <td>apparently shingles red spots left breast appe...</td>\n",
       "      <td>647</td>\n",
       "      <td>6.472346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the Docetaxel doing once a week x3 weeks th...</td>\n",
       "      <td>docetaxel week weeks week claim less harsh eff...</td>\n",
       "      <td>528</td>\n",
       "      <td>6.269096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  256 (previously stable on natalizumab), with 5...   \n",
       "1  On fingolimod and have been since December 201...   \n",
       "2  Apparently it's shingles! :-/ I do have a few ...   \n",
       "3  If the Docetaxel doing once a week x3 weeks th...   \n",
       "\n",
       "                                        Cleaned Text  length  log_length  \n",
       "0  previously stable natalizumab switching fingol...      50    3.912023  \n",
       "1  fingolimod since december way describe better ...      78    4.356709  \n",
       "2  apparently shingles red spots left breast appe...     647    6.472346  \n",
       "3  docetaxel week weeks week claim less harsh eff...     528    6.269096  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length</th>\n",
       "      <th>log_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reply posted for JessZidek. Hi Jess Sorry to r...</td>\n",
       "      <td>reply posted jesszidek hi jess sorry read chal...</td>\n",
       "      <td>0</td>\n",
       "      <td>401</td>\n",
       "      <td>5.993961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last Updated: January 01, 2017.  Share | Comme...</td>\n",
       "      <td>last updated january share comments tell frien...</td>\n",
       "      <td>0</td>\n",
       "      <td>14476</td>\n",
       "      <td>9.580247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi I was on rebif for about a year â rotate ...</td>\n",
       "      <td>hi rebif year rotate injection sites remember ...</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>5.087596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Reply posted for JessZidek. Hi Jess Sorry to r...   \n",
       "1  Last Updated: January 01, 2017.  Share | Comme...   \n",
       "2  Hi I was on rebif for about a year â rotate ...   \n",
       "\n",
       "                                        Cleaned Text sentiment  length  \\\n",
       "0  reply posted jesszidek hi jess sorry read chal...         0     401   \n",
       "1  last updated january share comments tell frien...         0   14476   \n",
       "2  hi rebif year rotate injection sites remember ...         0     162   \n",
       "\n",
       "   log_length  \n",
       "0    5.993961  \n",
       "1    9.580247  \n",
       "2    5.087596  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3671, 3)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "      <th>log_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reply posted jesszidek hi jess sorry read chal...</td>\n",
       "      <td>401</td>\n",
       "      <td>5.993961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>last updated january share comments tell frien...</td>\n",
       "      <td>14476</td>\n",
       "      <td>9.580247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi rebif year rotate injection sites remember ...</td>\n",
       "      <td>162</td>\n",
       "      <td>5.087596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Cleaned Text  length  log_length\n",
       "0  reply posted jesszidek hi jess sorry read chal...     401    5.993961\n",
       "1  last updated january share comments tell frien...   14476    9.580247\n",
       "2  hi rebif year rotate injection sites remember ...     162    5.087596"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1['log_length'] = np.log(X_train1['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['length'] = new123['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['log_length'] = np.log(final['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reply posted jesszidek hi jess sorry read chal...</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>last updated january share comments tell frien...</td>\n",
       "      <td>14476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi rebif year rotate injection sites remember ...</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Cleaned Text  length\n",
       "0  reply posted jesszidek hi jess sorry read chal...     401\n",
       "1  last updated january share comments tell frien...   14476\n",
       "2  hi rebif year rotate injection sites remember ...     162"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new123.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5245, 3), (5245, 2))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape, new123.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1['log_length'] = np.log(final1['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_bow = MultinomialNB(alpha = 0.0001)\n",
    "# fitting the model and predicting the responses\n",
    "om_bow.fit(a1, y_train1)\n",
    "ompredictions_bow  = om_bow.predict(b134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub8 = pd.DataFrame()\n",
    "df_sub8['unique_hash'] = test_df['unique_hash']\n",
    "df_sub8['sentiment'] = ompredictions_bow\n",
    "df_sub8.to_csv('my_submission8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (3671, 27066)\n",
      "the number of unique words : 27066\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer()\n",
    "c1 = tf_idf_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "d1 = tf_idf_vect.transform(final1['Cleaned Text'])\n",
    "print(\"the type of count vectorizer :\",type(c1))\n",
    "print(\"the shape of out text TFIDF vectorizer : \",c1.get_shape())\n",
    "print(\"the number of unique words :\", c1.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5245, 3671)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final['length']),len(X_train1['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = preprocessing.normalize(c1)\n",
    "c4 = sparse.csr_matrix(X_train1['log_length'].values)\n",
    "#c2 = preprocessing.normalize(c2)\n",
    "c5 = sparse.hstack([c1, c4.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256 (previously stable on natalizumab), with 5...</td>\n",
       "      <td>previously stable natalizumab switching fingol...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On fingolimod and have been since December 201...</td>\n",
       "      <td>fingolimod since december way describe better ...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apparently it's shingles! :-/ I do have a few ...</td>\n",
       "      <td>apparently shingles red spots left breast appe...</td>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  256 (previously stable on natalizumab), with 5...   \n",
       "1  On fingolimod and have been since December 201...   \n",
       "2  Apparently it's shingles! :-/ I do have a few ...   \n",
       "\n",
       "                                        Cleaned Text  length  \n",
       "0  previously stable natalizumab switching fingol...      50  \n",
       "1  fingolimod since december way describe better ...      78  \n",
       "2  apparently shingles red spots left breast appe...     647  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2924, 27066)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = preprocessing.normalize(d1)\n",
    "d135 = sparse.csr_matrix(final1['log_length'].values)\n",
    "#d135 = preprocessing.normalize(d135)\n",
    "d5 = sparse.hstack([d1, d135.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2924, 27067)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1574,)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1574, 2924]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-171-0134783590b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#model6.fit(a1, y_train1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#print(model6.best_estimator_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    426\u001b[0m                              % self.best_estimator_)\n\u001b[0;32m    427\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefit\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultimetric_\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n\u001b[1;32m---> 97\u001b[1;33m                                                  **self._kwargs)\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m   1057\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[0;32m   1058\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1059\u001b[1;33m                        sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m   1180\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1182\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1183\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1413\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[1;32m-> 1415\u001b[1;33m                                     pos_label)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                          str(average_options))\n\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'binary'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1574, 2924]"
     ]
    }
   ],
   "source": [
    "nb=MultinomialNB()\n",
    "parameters={'alpha':[0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\n",
    "model6=GridSearchCV(nb,parameters,cv=25, scoring = 'f1_micro', return_train_score= True)\n",
    "model6.fit(c1, y_train1)\n",
    "print(model6.best_estimator_)\n",
    "print(model6.score(d1, y_test1))\n",
    "#model6.fit(a1, y_train1)\n",
    "#print(model6.best_estimator_)\n",
    "#print(model6.score(b1, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n",
      "0.9440914866581956\n"
     ]
    }
   ],
   "source": [
    "nb=MultinomialNB()\n",
    "parameters={'alpha':[0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\n",
    "model6=GridSearchCV(nb,parameters,cv=25, scoring = 'f1_micro', return_train_score= True)\n",
    "model6.fit(c3, y_train1)\n",
    "print(model6.best_estimator_)\n",
    "print(model6.score(d3, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_bow = MultinomialNB(alpha = 0.1)\n",
    "# fitting the model and predicting the responses\n",
    "om_bow.fit(c3, y_train1)\n",
    "ompredictions_bow  = om_bow.predict(d136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub9 = pd.DataFrame()\n",
    "df_sub9['unique_hash'] = test_df['unique_hash']\n",
    "df_sub9['sentiment'] = ompredictions_bow\n",
    "df_sub9.to_csv('my_submission9.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "      <th>log_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reply posted jesszidek hi jess sorry read chal...</td>\n",
       "      <td>401</td>\n",
       "      <td>5.993961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>last updated january share comments tell frien...</td>\n",
       "      <td>14476</td>\n",
       "      <td>9.580247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi rebif year rotate injection sites remember ...</td>\n",
       "      <td>162</td>\n",
       "      <td>5.087596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem know hard lonely journey happy help...</td>\n",
       "      <td>785</td>\n",
       "      <td>6.665684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conclusion real life results suggest intravitr...</td>\n",
       "      <td>134</td>\n",
       "      <td>4.897840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Cleaned Text  length  log_length\n",
       "0  reply posted jesszidek hi jess sorry read chal...     401    5.993961\n",
       "1  last updated january share comments tell frien...   14476    9.580247\n",
       "2  hi rebif year rotate injection sites remember ...     162    5.087596\n",
       "3  no problem know hard lonely journey happy help...     785    6.665684\n",
       "4  conclusion real life results suggest intravitr...     134    4.897840"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "      <th>log_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>beth second ocrevus infusion yesterday nurses ...</td>\n",
       "      <td>326</td>\n",
       "      <td>5.786897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3672</th>\n",
       "      <td>diagnosed uc years ago total colectomy surgery...</td>\n",
       "      <td>624</td>\n",
       "      <td>6.436150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>rilonacept used treat symptoms rare genetic co...</td>\n",
       "      <td>5279</td>\n",
       "      <td>8.571492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3674</th>\n",
       "      <td>max dose lialda tablets day started flaring in...</td>\n",
       "      <td>1100</td>\n",
       "      <td>7.003065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>grandma think may find tad daunting quite hone...</td>\n",
       "      <td>170</td>\n",
       "      <td>5.135798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Cleaned Text  length  log_length\n",
       "3671  beth second ocrevus infusion yesterday nurses ...     326    5.786897\n",
       "3672  diagnosed uc years ago total colectomy surgery...     624    6.436150\n",
       "3673  rilonacept used treat symptoms rare genetic co...    5279    8.571492\n",
       "3674  max dose lialda tablets day started flaring in...    1100    7.003065\n",
       "3675  grandma think may find tad daunting quite hone...     170    5.135798"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1['log_length'] = np.log(X_test1['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (3671, 27066)\n",
      "the number of unique words : 27066\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer()\n",
    "c1 = tf_idf_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "d1 = tf_idf_vect.transform(X_testbow['Cleaned Text'])\n",
    "print(\"the type of count vectorizer :\",type(c1))\n",
    "print(\"the shape of out text TFIDF vectorizer : \",c1.get_shape())\n",
    "print(\"the number of unique words :\", c1.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = preprocessing.normalize(c1)\n",
    "c2 = sparse.csr_matrix(X_train1['length'].values)\n",
    "c2 = preprocessing.normalize(c2)\n",
    "c3 = sparse.hstack([c1, c2.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "c4 = sparse.csr_matrix(X_train1['log_length'].values)\n",
    "c5 = sparse.hstack([c1, c4.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = preprocessing.normalize(d1)\n",
    "d2 = sparse.csr_matrix(X_test1['length'].values)\n",
    "d2 = preprocessing.normalize(d2)\n",
    "d3 = sparse.hstack([d1, d2.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = sparse.csr_matrix(X_test1['log_length'].values)\n",
    "d5 = sparse.hstack([d1, d4.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.0001, class_prior=None, fit_prior=True)\n",
      "0.863405336721728\n"
     ]
    }
   ],
   "source": [
    "nb=MultinomialNB()\n",
    "parameters={'alpha':[0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\n",
    "model6=GridSearchCV(nb,parameters,cv=25, scoring = 'f1_micro', return_train_score= True)\n",
    "model6.fit(c5, y_train1)\n",
    "print(model6.best_estimator_)\n",
    "print(model6.score(d5, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramgrid = {'max_depth': list(range(1, 20, 2)), 'n_estimators': list(range(1, 200, 20))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-215-6b86e4e33302>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparamgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'f1_micro'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    664\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 666\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             train_scores = _score(estimator, X_train, y_train, scorer,\n\u001b[1;32m--> 558\u001b[1;33m                                   is_multimetric)\n\u001b[0m\u001b[0;32m    559\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[0;32m    595\u001b[0m     \"\"\"\n\u001b[0;32m    596\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[1;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[0;32m    625\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \"\"\"\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \"\"\"\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    599\u001b[0m             delayed(_accumulate_prediction)(e.predict_proba, X, all_proba,\n\u001b[0;32m    600\u001b[0m                                             lock)\n\u001b[1;32m--> 601\u001b[1;33m             for e in self.estimators_)\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mproba\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_proba\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_accumulate_prediction\u001b[1;34m(predict, X, out, lock)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[0mcomplains\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mit\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mplaced\u001b[0m \u001b[0mthere\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m     \"\"\"\n\u001b[1;32m--> 394\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    846\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tree_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mget_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[1;34m\"\"\"Get shape of a matrix.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model6 = GridSearchCV(RandomForestClassifier(random_state=1),paramgrid, scoring = 'f1_micro', cv= 5, return_train_score= True)\n",
    "model6.fit(c5, y_train1)\n",
    "print(model6.best_estimator_)\n",
    "print(model6.score(d5, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1574,), <2924x27067 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 335122 stored elements in COOrdinate format>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1.shape,d5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_rf = RandomForestClassifier(bootstrap=True, class_weight= 'balanced', criterion='gini',\n",
    "                       max_depth=19, max_features='auto', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=21,\n",
    "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
    "                       warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "      <th>log_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256 (previously stable on natalizumab), with 5...</td>\n",
       "      <td>previously stable natalizumab switching fingol...</td>\n",
       "      <td>50</td>\n",
       "      <td>3.912023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On fingolimod and have been since December 201...</td>\n",
       "      <td>fingolimod since december way describe better ...</td>\n",
       "      <td>78</td>\n",
       "      <td>4.356709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apparently it's shingles! :-/ I do have a few ...</td>\n",
       "      <td>apparently shingles red spots left breast appe...</td>\n",
       "      <td>647</td>\n",
       "      <td>6.472346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the Docetaxel doing once a week x3 weeks th...</td>\n",
       "      <td>docetaxel week weeks week claim less harsh eff...</td>\n",
       "      <td>528</td>\n",
       "      <td>6.269096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  256 (previously stable on natalizumab), with 5...   \n",
       "1  On fingolimod and have been since December 201...   \n",
       "2  Apparently it's shingles! :-/ I do have a few ...   \n",
       "3  If the Docetaxel doing once a week x3 weeks th...   \n",
       "\n",
       "                                        Cleaned Text  length  log_length  \n",
       "0  previously stable natalizumab switching fingol...      50    3.912023  \n",
       "1  fingolimod since december way describe better ...      78    4.356709  \n",
       "2  apparently shingles red spots left breast appe...     647    6.472346  \n",
       "3  docetaxel week weeks week claim less harsh eff...     528    6.269096  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (3671, 27066)\n",
      "the number of unique words : 27066\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer()\n",
    "c1 = tf_idf_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "d134 = tf_idf_vect.transform(final1['Cleaned Text'])\n",
    "print(\"the type of count vectorizer :\",type(c1))\n",
    "print(\"the shape of out text TFIDF vectorizer : \",c1.get_shape())\n",
    "print(\"the number of unique words :\", c1.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = preprocessing.normalize(c1)\n",
    "c2 = sparse.csr_matrix(X_train1['length'].values)\n",
    "c2 = preprocessing.normalize(c2)\n",
    "c3 = sparse.hstack([c1, c2.T])\n",
    "c4 = sparse.csr_matrix(X_train1['log_length'].values)\n",
    "c5 = sparse.hstack([c1, c4.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "d134 = preprocessing.normalize(d134)\n",
    "d135 = sparse.csr_matrix(final1['length'].values)\n",
    "d135 = preprocessing.normalize(d135)\n",
    "d136 = sparse.hstack([d134, d135.T])\n",
    "d137 = sparse.csr_matrix(final1['log_length'].values)\n",
    "d50 = sparse.hstack([d134, d137.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_rf.fit(c5, y_train1)\n",
    "ompredictions_rf  = om_rf.predict(d50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub11 = pd.DataFrame()\n",
    "df_sub11['unique_hash'] = test_df['unique_hash']\n",
    "df_sub11['sentiment'] = ompredictions_bow\n",
    "df_sub11.to_csv('my_submission11.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5245, 3)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new123.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "new123['log_length'] = np.log(new123['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(new123,final['sentiment'].values,test_size=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = final['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "sc2=0\n",
    "score1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14 of kfold 2\n",
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (2622, 23977)\n",
      "the number of unique words : 23977\n",
      "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.48378644817365535\n",
      "\n",
      "15 of kfold 2\n",
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (2623, 23775)\n",
      "the number of unique words : 23775\n",
      "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.5059418168930231\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=2,random_state=1,shuffle=True)\n",
    "for train1_index,test1_index in kf.split(new123,y1):\n",
    "     print('\\n{} of kfold {}'.format(i,kf.n_splits))\n",
    "     xtr,xvl = new123.loc[train1_index],new123.loc[test1_index]\n",
    "     ytr,yvl = y1[train1_index],y1[test1_index]\n",
    "    \n",
    "     tf_idf_vect = TfidfVectorizer()\n",
    "     c1 = tf_idf_vect.fit_transform(xtr['Cleaned Text'].values)     \n",
    "     d1 = tf_idf_vect.transform(xvl['Cleaned Text'])\n",
    "     print(\"the type of count vectorizer :\",type(c1))\n",
    "     print(\"the shape of out text TFIDF vectorizer : \",c1.get_shape())\n",
    "     print(\"the number of unique words :\", c1.get_shape()[1])\n",
    "     c1 = preprocessing.normalize(c1)\n",
    "     d1 = preprocessing.normalize(d1)\n",
    "     c2 = sparse.csr_matrix(xtr['length'].values)\n",
    "     c2 = preprocessing.normalize(c2)\n",
    "     c3 = sparse.hstack([c1, c2.T])\n",
    "     c4 = sparse.csr_matrix(xtr['log_length'].values)\n",
    "     c5 = sparse.hstack([c1, c4.T])\n",
    "     d2 = sparse.csr_matrix(xvl['length'].values)\n",
    "     d2 = preprocessing.normalize(d2)\n",
    "     d3 = sparse.hstack([d1, d2.T])   \n",
    "     d4 = sparse.csr_matrix(xvl['log_length'].values)\n",
    "     d5 = sparse.hstack([d1, d4.T])  \n",
    "     c5 = preprocessing.normalize(c5)\n",
    "     d5 = preprocessing.normalize(d5)   \n",
    "     #model = LogisticRegression(random_state=1)\n",
    "     #model.fit(xtr, ytr)\n",
    "     #pred_test = model.predict(xvl)\n",
    "     #score = accuracy_score(yvl,pred_test)\n",
    "     #print('accuracy_score',score)\n",
    "     tuned_parameters = [{'C': [0.0001,0.001, 0.01,0.1,1,10,100,1000,10000]}]\n",
    "     model1 = GridSearchCV(LogisticRegression(solver = 'lbfgs', class_weight = 'balanced' ,multi_class='multinomial', max_iter = 1500), tuned_parameters, scoring = 'f1_macro', cv=5, return_train_score= True)\n",
    "     model1.fit(c3, ytr)\n",
    "     print(model1.best_estimator_)\n",
    "     score = model1.score(d3, yvl)\n",
    "     print(score)\n",
    "     score1 = score + score1   \n",
    "     i+=1\n",
    "#pred_test_sk = model1.predict(d50)\n",
    "#pred_sk=model.predict_proba(xvl)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'f1_micro', cv=5, return_train_score= True)\n",
    "     model1.fit(c3, ytr)\n",
    "     print(model1.best_estimator_)\n",
    "     score = model1.score(d3, yvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average Accuracy is 1.4022819176057744\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Average Accuracy is {}\" .format(score1/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunSVC(X_Train,X_Test,y_train,y_test,Search_Type):    \n",
    "    #lb_make = LabelEncoder()\n",
    "    \n",
    "    #y_train_encoded = lb_make.fit_transform(y_train)\n",
    "    #y_test_encoded = lb_make.fit_transform(y_test)\n",
    "    \n",
    "    \n",
    "    if (Search_Type == 'grid'):\n",
    "        grid_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 100, 1000]}]\n",
    "        model = GridSearchCV(SVC(class_weight = 'balanced'),grid_parameters,cv = 5,scoring = 'f1_micro')\n",
    "        model.fit(X_Train,y_train)\n",
    "        print(model.best_estimator_)\n",
    "        print('The Score with '+ Search_Type+ 'search CV is: '+ str(model.score(X_Test, y_test)))\n",
    "    elif (Search_Type == 'random'):\n",
    "        random_parameters = dict(C=[1, 10, 100, 1000],gamma=[1e-3, 1e-4])  \n",
    "        model = GridSearchCV(SVC(class_weight = 'balanced'),random_parameters,cv = 5,scoring = 'f1_micro',n_jobs= 1)\n",
    "        model.fit(X_Train,y_train)\n",
    "        print(model.best_estimator_)\n",
    "        print('The Score with '+ Search_Type+ 'search CV is: ' + str(model.score(X_Test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (3671, 27066)\n",
      "the number of unique words : 27066\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer()\n",
    "c1 = tf_idf_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "d1 = tf_idf_vect.transform(X_testbow['Cleaned Text'])\n",
    "print(\"the type of count vectorizer :\",type(c1))\n",
    "print(\"the shape of out text TFIDF vectorizer : \",c1.get_shape())\n",
    "print(\"the number of unique words :\", c1.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = preprocessing.normalize(c1)\n",
    "c2 = sparse.csr_matrix(X_train1['length'].values)\n",
    "c2 = preprocessing.normalize(c2)\n",
    "c3 = sparse.hstack([c1, c2.T])\n",
    "d1 = preprocessing.normalize(d1)\n",
    "d2 = sparse.csr_matrix(X_test1['length'].values)\n",
    "d2 = preprocessing.normalize(d2)\n",
    "d3 = sparse.hstack([d1, d2.T])\n",
    "c4 = sparse.csr_matrix(X_train1['log_length'].values)\n",
    "c5 = sparse.hstack([c1, c4.T])\n",
    "d4 = sparse.csr_matrix(X_test1['log_length'].values)\n",
    "d5 = sparse.hstack([d1, d4.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "c5 = preprocessing.normalize(c5)\n",
    "d5 = preprocessing.normalize(d5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3671, 1574, (3671, 27067), (1574, 27067))"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train1),len(y_test1),c5.shape,d5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "The Score with gridsearch CV is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VishnuVardhan\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "The Score with randomsearch CV is: 1.0\n"
     ]
    }
   ],
   "source": [
    "RunSVC(c5,d5,y_train1,y_test1,'grid')\n",
    "RunSVC(c5,d5,y_train1,y_test1,'random')\n",
    "#RunSVC(X_Train_vectorised,X_Test_vectorised,y_train,y_test,'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (3671, 6000)\n",
      "the number of unique words : 6000\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer(max_features= 6000, min_df=7, max_df=0.9)\n",
    "c1 = tf_idf_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "d134 = tf_idf_vect.transform(final1['Cleaned Text'])\n",
    "print(\"the type of count vectorizer :\",type(c1))\n",
    "print(\"the shape of out text TFIDF vectorizer : \",c1.get_shape())\n",
    "print(\"the number of unique words :\", c1.get_shape()[1])\n",
    "\n",
    "c1 = preprocessing.normalize(c1)\n",
    "c2 = sparse.csr_matrix(X_train1['length'].values)\n",
    "c2 = preprocessing.normalize(c2)\n",
    "c3 = sparse.hstack([c1, c2.T])\n",
    "c4 = sparse.csr_matrix(X_train1['log_length'].values)\n",
    "c5 = sparse.hstack([c1, c4.T])\n",
    "\n",
    "\n",
    "d134 = preprocessing.normalize(d134)\n",
    "d135 = sparse.csr_matrix(final1['length'].values)\n",
    "d135 = preprocessing.normalize(d135)\n",
    "d136 = sparse.hstack([d134, d135.T])\n",
    "d137 = sparse.csr_matrix(final1['log_length'].values)\n",
    "d50 = sparse.hstack([d134, d137.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect = TfidfVectorizer(max_features= 6000, min_df=7, max_df=0.9)\n",
    "#c1 = tf_idf_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "d134 = tf_idf_vect.fit_transform(final1['Cleaned Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA.fit(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA.fit(d134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro\n",
      "looked\n",
      "md\n",
      "key\n",
      "physicians\n",
      "tags\n",
      "enlarging\n",
      "reoccurrence\n",
      "thousand\n",
      "butter\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(tf_idf_vect.get_feature_names()))\n",
    "    print(tf_idf_vect.get_feature_names()[random_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_topic = LDA.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_topic1 = LDA.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topic_words1 = first_topic1.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topic_words = first_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tumor\n",
      "radiation\n",
      "stage\n",
      "tagrisso\n",
      "mets\n",
      "lung\n",
      "chemo\n",
      "cancer\n",
      "alimta\n",
      "tarceva\n"
     ]
    }
   ],
   "source": [
    "for i in top_topic_words:\n",
    "    print(tf_idf_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['tumor', 'radiation', 'stage', 'tagrisso', 'mets', 'lung', 'chemo', 'cancer', 'alimta', 'tarceva']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['trial', 'multiple', 'ocrelizumab', 'disease', 'cells', 'treatment', 'cancer', 'study', 'ms', 'patients']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['no', 'good', 'like', 'years', 'ocrevus', 'ms', 'get', 'would', 'gilenya', 'not']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['pred', 'glutamine', 'march', 'vsl', 'diverticulitis', 'pancolitis', 'folic', 'uceris', 'hosp', 'apriso']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['colectomy', 'stelara', 'yay', 'azathioprine', 'chronic', 'meds', 'subtotal', 'ileostomy', 'surgery', 'current']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tf_idf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2924, 5)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_values1 = LDA.transform(d134)\n",
    "topic_values1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3671, 5)"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_values = LDA.transform(c1)\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1['Topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1['Topic'] = topic_values1.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "      <th>log_length</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256 (previously stable on natalizumab), with 5...</td>\n",
       "      <td>previously stable natalizumab switching fingol...</td>\n",
       "      <td>50</td>\n",
       "      <td>3.912023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On fingolimod and have been since December 201...</td>\n",
       "      <td>fingolimod since december way describe better ...</td>\n",
       "      <td>78</td>\n",
       "      <td>4.356709</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apparently it's shingles! :-/ I do have a few ...</td>\n",
       "      <td>apparently shingles red spots left breast appe...</td>\n",
       "      <td>647</td>\n",
       "      <td>6.472346</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  256 (previously stable on natalizumab), with 5...   \n",
       "1  On fingolimod and have been since December 201...   \n",
       "2  Apparently it's shingles! :-/ I do have a few ...   \n",
       "\n",
       "                                        Cleaned Text  length  log_length  \\\n",
       "0  previously stable natalizumab switching fingol...      50    3.912023   \n",
       "1  fingolimod since december way describe better ...      78    4.356709   \n",
       "2  apparently shingles red spots left breast appe...     647    6.472346   \n",
       "\n",
       "   Topic  \n",
       "0      1  \n",
       "1      4  \n",
       "2      4  "
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>length</th>\n",
       "      <th>log_length</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reply posted jesszidek hi jess sorry read chal...</td>\n",
       "      <td>401</td>\n",
       "      <td>5.993961</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>last updated january share comments tell frien...</td>\n",
       "      <td>14476</td>\n",
       "      <td>9.580247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi rebif year rotate injection sites remember ...</td>\n",
       "      <td>162</td>\n",
       "      <td>5.087596</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Cleaned Text  length  log_length  \\\n",
       "0  reply posted jesszidek hi jess sorry read chal...     401    5.993961   \n",
       "1  last updated january share comments tell frien...   14476    9.580247   \n",
       "2  hi rebif year rotate injection sites remember ...     162    5.087596   \n",
       "\n",
       "   Topic  \n",
       "0      2  \n",
       "1      1  \n",
       "2      2  "
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3671, 4), (2924, 5))"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape, final1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer :  (3671, 6000)\n",
      "the number of unique words : 6000\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer(max_features = 6000,min_df=7, max_df=0.9)\n",
    "c1 = tf_idf_vect.fit_transform(X_trainbow['Cleaned Text'].values)\n",
    "d134 = tf_idf_vect.transform(final1['Cleaned Text'])\n",
    "print(\"the type of count vectorizer :\",type(c1))\n",
    "print(\"the shape of out text TFIDF vectorizer : \",c1.get_shape())\n",
    "print(\"the number of unique words :\", c1.get_shape()[1])\n",
    "\n",
    "#c1 = preprocessing.normalize(c1)\n",
    "c2 = sparse.csr_matrix(X_train1['length'].values)\n",
    "#c2 = preprocessing.normalize(c2)\n",
    "c3 = sparse.hstack([c1, c2.T])\n",
    "c4 = sparse.csr_matrix(X_train1['log_length'].values)\n",
    "c5 = sparse.hstack([c1, c4.T])\n",
    "c6 = sparse.csr_matrix(X_train1['Topic'].values)\n",
    "c7 = sparse.hstack([c5, c6.T])\n",
    "c8 = sparse.hstack([c7, c2.T])\n",
    "#d134 = preprocessing.normalize(d134)\n",
    "d135 = sparse.csr_matrix(final1['length'].values)\n",
    "#d135 = preprocessing.normalize(d135)\n",
    "d136 = sparse.hstack([d134, d135.T])\n",
    "d137 = sparse.csr_matrix(final1['log_length'].values)\n",
    "d50 = sparse.hstack([d134, d137.T])\n",
    "d60 = sparse.csr_matrix(final1['Topic'].values)\n",
    "d70 = sparse.hstack([d50, d60.T])\n",
    "d80 = sparse.hstack([d70, d135.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "c3 = preprocessing.normalize(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "d136 = preprocessing.normalize(d136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_rf = SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_rf = RandomForestClassifier(bootstrap=True, class_weight= 'balanced', criterion='gini',\n",
    "                       max_depth=19, max_features='auto', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=21,\n",
    "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
    "                       warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_rf = LogisticRegression(C= 1.95, class_weight='balanced', dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter= 5000,\n",
    "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.95, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=5000, multi_class='multinomial', n_jobs=None,\n",
       "                   penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "                   verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "om_rf.fit(c3, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c1 = preprocessing.normalize(c1)\n",
    "#d134 = preprocessing.normalize(d134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "ompredictions_rf  = om_rf.predict(d136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub22 = pd.DataFrame()\n",
    "df_sub22['unique_hash'] = test_df['unique_hash']\n",
    "df_sub22['sentiment'] = ompredictions_rf\n",
    "df_sub22.to_csv('my_submission22.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1550\n",
       "1     787\n",
       "0     587\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub22['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub21 = pd.DataFrame()\n",
    "df_sub21['unique_hash'] = test_df['unique_hash']\n",
    "df_sub21['sentiment'] = ompredictions_rf\n",
    "df_sub21.to_csv('my_submission21.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1612\n",
       "1     735\n",
       "0     577\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub21['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub20 = pd.DataFrame()\n",
    "df_sub20['unique_hash'] = test_df['unique_hash']\n",
    "df_sub20['sentiment'] = ompredictions_rf\n",
    "df_sub20.to_csv('my_submission20.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1673\n",
       "1     653\n",
       "0     598\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub20['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub19 = pd.DataFrame()\n",
    "df_sub19['unique_hash'] = test_df['unique_hash']\n",
    "df_sub19['sentiment'] = ompredictions_rf\n",
    "df_sub19.to_csv('my_submission19.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1681\n",
       "1     650\n",
       "0     593\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub19['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub18 = pd.DataFrame()\n",
    "df_sub18['unique_hash'] = test_df['unique_hash']\n",
    "df_sub18['sentiment'] = ompredictions_rf\n",
    "df_sub18.to_csv('my_submission18.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub15 = pd.DataFrame()\n",
    "df_sub15['unique_hash'] = test_df['unique_hash']\n",
    "df_sub15['sentiment'] = ompredictions_rf\n",
    "df_sub15.to_csv('my_submission15.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1670\n",
       "1     707\n",
       "0     547\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub18['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1548\n",
       "1     741\n",
       "0     635\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub13['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    2924\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub14['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1671\n",
       "1     707\n",
       "0     546\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub15['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1548\n",
       "1     741\n",
       "0     635\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub16['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(c3, ytr)\n",
    "print(model1.best_estimator_)\n",
    "score = model1.score(d3, yvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import expon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
